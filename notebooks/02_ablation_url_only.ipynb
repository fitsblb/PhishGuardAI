{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c04bf5e",
   "metadata": {},
   "source": [
    "## **Ablation - URL-only manifest**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db06cd0",
   "metadata": {},
   "source": [
    "### **SECTION 0: Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "import joblib\n",
    "import hashlib\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae2b2e8",
   "metadata": {},
   "source": [
    "- **Set working directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc490286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: d:\\MLops\\NetworkSecurity\n",
      "[feature_extraction] Loaded 1401 TLD probabilities\n"
     ]
    }
   ],
   "source": [
    "# Set working directory to project root\n",
    "if Path.cwd().name == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Add src to path so we can import common modules\n",
    "sys.path.insert(0, str(Path.cwd() / \"src\"))\n",
    "from common.feature_extraction import extract_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a797b9",
   "metadata": {},
   "source": [
    "### **SECTION 1: Set up path & define constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677b791d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n",
      "python-dotenv could not parse statement starting at line 3\n",
      "python-dotenv could not parse statement starting at line 7\n",
      "python-dotenv could not parse statement starting at line 10\n",
      "python-dotenv could not parse statement starting at line 11\n",
      "python-dotenv could not parse statement starting at line 12\n",
      "python-dotenv could not parse statement starting at line 13\n",
      "python-dotenv could not parse statement starting at line 14\n",
      "python-dotenv could not parse statement starting at line 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL TRAINING: OPTIMAL 8-FEATURE URL-ONLY BASELINE\n",
      "============================================================\n",
      "\n",
      "Configuration:\n",
      "  Data: data\\processed\\phiusiil_features_v2.csv\n",
      "  Features: 8\n",
      "  Seed: 42\n",
      "  MLflow: http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "SEED = 42\n",
    "DATA_PATH = Path(\"data/processed/phiusiil_features_v2.csv\")\n",
    "THRESH_PATH = Path(\"configs/dev/thresholds.json\")\n",
    "MLFLOW_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://localhost:5000\")\n",
    "EXPERIMENT = os.getenv(\"MLFLOW_EXPERIMENT\", \"phiusiil_baselines\")\n",
    "THRESH_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "OPTIMAL_FEATURES = [\n",
    "    \"IsHTTPS\",\n",
    "    \"TLDLegitimateProb\",\n",
    "    \"CharContinuationRate\",\n",
    "    \"SpacialCharRatioInURL\",\n",
    "    \"URLCharProb\",\n",
    "    \"LetterRatioInURL\",\n",
    "    \"NoOfOtherSpecialCharsInURL\",\n",
    "    \"DomainLength\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL TRAINING: OPTIMAL 8-FEATURE URL-ONLY BASELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Data: {DATA_PATH}\")\n",
    "print(f\"  Features: {len(OPTIMAL_FEATURES)}\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  MLflow: {MLFLOW_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b74e23",
   "metadata": {},
   "source": [
    "### **SECTION 2: Load & Validate Data**\n",
    "-  Load the final feature set from EDA and validate it matches expectations. This prevents silent failures if the data file is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3582563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOAD & VALIDATE FEATURE SET\n",
      "============================================================\n",
      "\n",
      "Loaded data shape: (234764, 10)\n",
      "Expected: (234764, 9)\n",
      "\n",
      "Warning: Extra columns in dataset (will be ignored): ['URL']\n",
      "\n",
      "Feature validation:\n",
      " IsHTTPS: float64, 0 nulls\n",
      " TLDLegitimateProb: float64, 0 nulls\n",
      " CharContinuationRate: float64, 0 nulls\n",
      " SpacialCharRatioInURL: float64, 0 nulls\n",
      " URLCharProb: float64, 0 nulls\n",
      " LetterRatioInURL: float64, 0 nulls\n",
      " NoOfOtherSpecialCharsInURL: int64, 0 nulls\n",
      " DomainLength: int64, 0 nulls\n",
      "\n",
      "Final feature matrix: (234764, 8)\n",
      "Label distribution:\n",
      "  Legitimate (1): 134850 (57.4%)\n",
      "  Phishing (0): 99914 (42.6%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOAD & VALIDATE FEATURE SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "assert DATA_PATH.exists(), f\"Missing data file: {DATA_PATH}\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"\\nLoaded data shape: {df.shape}\")\n",
    "print(f\"Expected: ({len(df)}, {len(OPTIMAL_FEATURES) + 1})\")  # +1 for label\n",
    "\n",
    "missing_features = [f for f in OPTIMAL_FEATURES if f not in df.columns]\n",
    "if missing_features:\n",
    "    raise ValueError(f\"Missing features in dataset: {missing_features}\")\n",
    "\n",
    "assert \"label\" in df.columns, \"Missing 'label' column\"\n",
    "\n",
    "extra_features = [c for c in df.columns if c not in OPTIMAL_FEATURES + [\"label\"]]\n",
    "if extra_features:\n",
    "    print(\n",
    "        f\"\\nWarning: Extra columns in dataset (will be ignored): {extra_features}\"\n",
    "    )  # URL will be ignored\n",
    "\n",
    "print(\"\\nFeature validation:\")\n",
    "for feature in OPTIMAL_FEATURES:\n",
    "    nulls = df[feature].isna().sum()\n",
    "    dtype = df[feature].dtype\n",
    "    print(f\" {feature}: {dtype}, {nulls} nulls\")\n",
    "\n",
    "X = df[OPTIMAL_FEATURES].copy()\n",
    "y = df[\"label\"].values\n",
    "\n",
    "print(f\"\\nFinal feature matrix: {X.shape}\")\n",
    "print(f\"Label distribution:\")\n",
    "print(f\"  Legitimate (1): {(y == 1).sum()} ({(y == 1).mean():.1%})\")\n",
    "print(f\"  Phishing (0): {(y == 0).sum()} ({(y == 0).mean():.1%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f12332",
   "metadata": {},
   "source": [
    "### **SECTION 3: Data splitting and Model training**\n",
    "- Purpose: Split data into training and validation sets with stratification to maintain class balance. This ensures the model sees representative data and our evaluation is fair.\n",
    "- Explanation:\n",
    "\n",
    "    - No Temporal order found in the dataset therefore no time series split, Uses sklean train,test,split\n",
    "    - Uses 80/20 train/validation split, standard for this dataset size\n",
    "    - stratify=y ensures both sets have same class distribution as full dataset\n",
    "    - random_state=SEED makes split reproducible for debugging\n",
    "    - Prints detailed class distributions to verify stratification worked correctly\n",
    "    - Also takes care of deduplication make sure no duplicates leak or confuse the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee015b8e",
   "metadata": {},
   "source": [
    "#### **3.1. Train/Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7846af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAIN/VALIDATION SPLIT\n",
      "============================================================\n",
      "\n",
      "Training set:\n",
      "  Samples: 187,811\n",
      "  Phishing: 79,931 (42.6%)\n",
      "  Legitimate: 107,880 (57.4%)\n",
      "\n",
      "Validation set:\n",
      "  Samples: 46,953\n",
      "  Phishing: 19,983 (42.6%)\n",
      "  Legitimate: 26,970 (57.4%)\n",
      "\n",
      "Feature matrix shapes:\n",
      "  X_train: (187811, 8)\n",
      "  X_val: (46953, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAIN/VALIDATION SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Samples: {len(X_train):,}\")\n",
    "print(f\"  Phishing: {(y_train == 0).sum():,} ({(y_train == 0).mean():.1%})\")\n",
    "print(f\"  Legitimate: {(y_train == 1).sum():,} ({(y_train == 1).mean():.1%})\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Samples: {len(X_val):,}\")\n",
    "print(f\"  Phishing: {(y_val == 0).sum():,} ({(y_val == 0).mean():.1%})\")\n",
    "print(f\"  Legitimate: {(y_val == 1).sum():,} ({(y_val == 1).mean():.1%})\")\n",
    "\n",
    "print(f\"\\nFeature matrix shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d141cc",
   "metadata": {},
   "source": [
    "#### **3.2. Model Training & Calibration**\n",
    "- Purpose: Train candidate models (LogisticRegression, XGBoost) and calibrate their probabilities using isotonic regression. Calibration ensures p_malicious values are reliable for threshold-based decisions\n",
    "- Explanation:\n",
    "\n",
    "    - Defines two baseline models: LogisticRegression (interpretable) and XGBoost (powerful)\n",
    "    - LogReg uses StandardScaler and class_weight=\"balanced\" to handle slight imbalance\n",
    "    - XGBoost uses shallow trees (depth=6) and regularization to avoid overfitting\n",
    "    - fit_calibrated() wraps model training with isotonic calibration (5-fold CV)\n",
    "    - Calibration corrects probability estimates so p_malicious is reliable for thresholding\n",
    "    - Selects best model by PR-AUC (primary) then F1-macro (tiebreaker)\n",
    "    - Stores all models and predictions for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80bd21b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL TRAINING & CALIBRATION\n",
      "============================================================\n",
      "\n",
      "Training logreg...\n",
      "  Model classes: [0 1]\n",
      "  Phishing prob column: 0, Legitimate prob column: 1\n",
      "  Sample p_mal: [0.00862244 1.         0.00589575] | Sample p_legit: [0.99137756 0.         0.99410425]\n",
      "  Class 0 (phishing) is at index: 0\n",
      "  Class 1 (legit) is at index: 1\n",
      "  Test sample features: {'IsHTTPS': 1.0, 'TLDLegitimateProb': 0.612, 'CharContinuationRate': 0.16, 'SpacialCharRatioInURL': 0.1923076923076923, 'URLCharProb': 0.06, 'LetterRatioInURL': 0.8076923076923077, 'NoOfOtherSpecialCharsInURL': 5, 'DomainLength': 18}\n",
      "  Training sample prediction: [[0.00713379 0.99286621]]\n",
      "  Sum of probabilities: 1.000000\n",
      "  P(phishing) for training sample: 0.007134\n",
      "  P(legit) for training sample: 0.992866\n",
      "  PR-AUC (phishing): 0.9965\n",
      "  F1-macro @0.5: 0.9848\n",
      "  Brier score: 0.011757\n",
      "\n",
      "Training xgb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MLops\\NetworkSecurity\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "d:\\MLops\\NetworkSecurity\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "d:\\MLops\\NetworkSecurity\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "d:\\MLops\\NetworkSecurity\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "d:\\MLops\\NetworkSecurity\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model classes: [0 1]\n",
      "  Phishing prob column: 0, Legitimate prob column: 1\n",
      "  Sample p_mal: [0.00815403 1.         0.00186653] | Sample p_legit: [0.99184597 0.         0.99813347]\n",
      "  Class 0 (phishing) is at index: 0\n",
      "  Class 1 (legit) is at index: 1\n",
      "  Test sample features: {'IsHTTPS': 1.0, 'TLDLegitimateProb': 0.612, 'CharContinuationRate': 0.16, 'SpacialCharRatioInURL': 0.1923076923076923, 'URLCharProb': 0.06, 'LetterRatioInURL': 0.8076923076923077, 'NoOfOtherSpecialCharsInURL': 5, 'DomainLength': 18}\n",
      "  Training sample prediction: [[0.00232477 0.99767523]]\n",
      "  Sum of probabilities: 1.000000\n",
      "  P(phishing) for training sample: 0.002325\n",
      "  P(legit) for training sample: 0.997675\n",
      "  PR-AUC (phishing): 0.9992\n",
      "  F1-macro @0.5: 0.9970\n",
      "  Brier score: 0.002637\n",
      "\n",
      "============================================================\n",
      "MODEL SELECTION\n",
      "============================================================\n",
      "\n",
      "Selected model: xgb\n",
      "  PR-AUC: 0.9992\n",
      "  F1-macro: 0.9970\n",
      "  Brier: 0.002637\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL TRAINING & CALIBRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "logreg_base = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"scaler\",\n",
    "            StandardScaler(with_mean=False),\n",
    "        ),  # to preserve range and avoid negative values\n",
    "        (\n",
    "            \"clf\",\n",
    "            LogisticRegression(\n",
    "                max_iter=2000, class_weight=\"balanced\", random_state=SEED\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "xgb_base = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=SEED,\n",
    "    n_jobs=0,\n",
    "    objective=\"binary:logistic\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "candidates = {\"logreg\": logreg_base, \"xgb\": xgb_base}\n",
    "\n",
    "\n",
    "def fit_calibrated(name, model):\n",
    "    \"\"\"\n",
    "    Train model and apply isotonic calibration via 5-fold CV.\n",
    "    Returns calibrated model, metrics dict, and p_malicious predictions.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    calib = CalibratedClassifierCV(\n",
    "        model,\n",
    "        method=\"isotonic\",\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED),\n",
    "    )\n",
    "    calib.fit(X_train, y_train)\n",
    "\n",
    "    # FIXED: Safe probability extraction that handles any class ordering\n",
    "    classes = calib.classes_\n",
    "    phish_idx = list(classes).index(0)  # Find index of phishing class (label=0)\n",
    "    legit_idx = list(classes).index(1)  # Find index of legitimate class (label=1)\n",
    "\n",
    "    # Extract probabilities directly from correct columns\n",
    "    proba_matrix = calib.predict_proba(X_val)\n",
    "    p_mal = proba_matrix[:, phish_idx]  # Direct P(phishing) from correct column\n",
    "    p_legit = proba_matrix[:, legit_idx]  # Direct P(legitimate) from correct column\n",
    "\n",
    "    # Validation: Ensure probabilities sum to 1.0 (sanity check)\n",
    "    prob_sum_check = np.allclose(p_mal + p_legit, 1.0)\n",
    "    if not prob_sum_check:\n",
    "        raise ValueError(f\"Probability columns don't sum to 1.0! Classes: {classes}\")\n",
    "\n",
    "    # Debug info\n",
    "    print(f\"  Model classes: {classes}\")\n",
    "    print(f\"  Phishing prob column: {phish_idx}, Legitimate prob column: {legit_idx}\")\n",
    "    print(f\"  Sample p_mal: {p_mal[:3]} | Sample p_legit: {p_legit[:3]}\")\n",
    "\n",
    "    # Additional verification logic - check class ordering and probability extraction\n",
    "    print(f\"  Class 0 (phishing) is at index: {list(calib.classes_).index(0)}\")\n",
    "    print(f\"  Class 1 (legit) is at index: {list(calib.classes_).index(1)}\")\n",
    "\n",
    "    # Test on a training sample to verify probability extraction\n",
    "    test_sample = X_train.iloc[[0]]\n",
    "    print(f\"  Test sample features: {test_sample.to_dict(orient='records')[0]}\")\n",
    "    test_proba = calib.predict_proba(test_sample)\n",
    "    print(f\"  Training sample prediction: {test_proba}\")\n",
    "    print(f\"  Sum of probabilities: {test_proba.sum():.6f}\")  # Should be 1.0\n",
    "    print(f\"  P(phishing) for training sample: {test_proba[0, phish_idx]:.6f}\")\n",
    "    print(f\"  P(legit) for training sample: {test_proba[0, legit_idx]:.6f}\")\n",
    "\n",
    "    # Make binary predictions at 0.5 threshold on p_malicious\n",
    "    y_hat_phish = (p_mal >= 0.5).astype(int)  # 1 = predict phishing\n",
    "    y_pred = 1 - y_hat_phish  # Convert to label space (0=phish, 1=legit)\n",
    "\n",
    "    # Compute metrics\n",
    "    f1m = f1_score(y_val, y_pred, average=\"macro\")\n",
    "    prauc = average_precision_score((y_val == 0).astype(int), p_mal)\n",
    "    brier = brier_score_loss((y_val == 0).astype(int), p_mal)\n",
    "\n",
    "    print(f\"  PR-AUC (phishing): {prauc:.4f}\")\n",
    "    print(f\"  F1-macro @0.5: {f1m:.4f}\")\n",
    "    print(f\"  Brier score: {brier:.6f}\")\n",
    "\n",
    "    return (\n",
    "        calib,\n",
    "        {\n",
    "            \"f1_macro@0.5_on_p_mal\": float(f1m),\n",
    "            \"pr_auc_phish\": float(prauc),\n",
    "            \"brier_phish\": float(brier),\n",
    "        },\n",
    "        p_mal,\n",
    "    )\n",
    "\n",
    "\n",
    "results = {}\n",
    "calibrated_models = {}\n",
    "p_malicious_preds = {}\n",
    "\n",
    "for name, model in candidates.items():\n",
    "    clf, metrics, p_mal = fit_calibrated(name, model)\n",
    "    calibrated_models[name] = clf\n",
    "    p_malicious_preds[name] = p_mal\n",
    "    results[name] = metrics\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sorted_models = sorted(\n",
    "    results.items(),\n",
    "    key=lambda kv: (kv[1][\"pr_auc_phish\"], kv[1][\"f1_macro@0.5_on_p_mal\"]),\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "best_name, best_metrics = sorted_models[0]\n",
    "best_model = calibrated_models[best_name]\n",
    "p_mal = p_malicious_preds[best_name]\n",
    "\n",
    "\n",
    "print(f\"\\nSelected model: {best_name}\")\n",
    "print(f\"  PR-AUC: {best_metrics['pr_auc_phish']:.4f}\")\n",
    "print(f\"  F1-macro: {best_metrics['f1_macro@0.5_on_p_mal']:.4f}\")\n",
    "print(f\"  Brier: {best_metrics['brier_phish']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d3d968",
   "metadata": {},
   "source": [
    "### **SECTION 4: Threshold Optimization + Gray-Zone Judge**\n",
    "- Find the optimal decision threshold t_star (F1-macro on validation), then locate a gray-zone band around it for judge routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d03baaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENHANCED THRESHOLD OPTIMIZATION WITH JUDGE INTEGRATION\n",
      "============================================================\n",
      "\n",
      "1. Standard F1-macro threshold optimization:\n",
      "  t_star: 0.350\n",
      "  F1-macro @t_star: 0.9972\n",
      "THRESHOLD OPTIMIZATION RESULTS:\n",
      "==================================================\n",
      "\n",
      "1. Optimal decision threshold:\n",
      "  t_star: 0.350\n",
      "  F1-macro @t_star: 0.0028\n",
      "\n",
      "2. Standard gray-zone band (target: 10-15%):\n",
      "  Low threshold: 0.004\n",
      "  High threshold: 0.999\n",
      "  Gray-zone rate: 10.9%\n",
      "\n",
      "3. Standard decision distribution:\n",
      "  ALLOW: 48.1% (22,584 samples)\n",
      "  REVIEW: 10.9% (5,135 samples)\n",
      "  BLOCK: 41.0% (19,234 samples)\n",
      "\n",
      "4. Enhanced Decision Logic Examples:\n",
      "----------------------------------------\n",
      "  URL: https://images.google.com                | Decision: REVIEW | Uncertain classification (p=0.250)\n",
      "  URL: https://github.com/user/repo             | Decision: ALLOW  | Well-known domain override (p=0.350)\n",
      "  URL: https://suspicious-bank-login.com        | Decision: REVIEW | Uncertain classification (p=0.450)\n",
      "  URL: https://definitely-phishing-site.evil    | Decision: REVIEW | Uncertain classification (p=0.850)\n",
      "  URL: https://legitimate-company.com           | Decision: REVIEW | Uncertain classification (p=0.150)\n",
      "\n",
      "5. Configuration saved to: configs\\dev\\thresholds.json\n",
      "\n",
      "Threshold optimization complete! ‚úì\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# SECTION 4: THRESHOLD OPTIMIZATION & JUDGE INTEGRATION\n",
    "# ========================================\n",
    "\"\"\"\n",
    "**Purpose:** Find optimal decision thresholds and implement enhanced routing logic for edge cases.\n",
    "\n",
    "**Enhanced Strategy:** \n",
    "- Standard F1-optimized thresholds for typical URLs\n",
    "- Judge-based routing for short domains (addresses github.com/google.com misclassification)\n",
    "- Three-tier decision framework: ALLOW/REVIEW/BLOCK\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENHANCED THRESHOLD OPTIMIZATION WITH JUDGE INTEGRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Standard F1-macro threshold optimization:\")\n",
    "grid = np.linspace(0.05, 0.95, 19)\n",
    "f1_scores = []\n",
    "\n",
    "for t in grid:\n",
    "    y_hat = (p_mal >= t).astype(int)  # Predict phishing if p_mal >= t\n",
    "    y_pred = 1 - y_hat  # Convert to label space\n",
    "    f1_scores.append(f1_score(y_val, y_pred, average=\"macro\"))\n",
    "\n",
    "t_star = float(grid[np.argmax(f1_scores)])\n",
    "best_f1 = max(f1_scores)\n",
    "\n",
    "print(f\"  t_star: {t_star:.3f}\")\n",
    "print(f\"  F1-macro @t_star: {best_f1:.4f}\")\n",
    "\n",
    "\n",
    "def pick_band_for_target(predictions, optimal_threshold, target=0.12, step=0.001):\n",
    "    \"\"\"\n",
    "    Pick gray-zone thresholds to achieve target gray-zone rate.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: Model probability predictions\n",
    "    - optimal_threshold: The optimal decision threshold (t_star)\n",
    "    - target: Target gray-zone rate (default 12%)\n",
    "    - step: Search step size\n",
    "\n",
    "    Returns:\n",
    "    - low_threshold, high_threshold, actual_gray_rate\n",
    "    \"\"\"\n",
    "    best_low, best_high, best_rate = None, None, float(\"inf\")\n",
    "\n",
    "    # Search for thresholds that give us closest to target rate\n",
    "    for low in np.arange(0.001, optimal_threshold, step):\n",
    "        for high in np.arange(optimal_threshold, 1.0, step):\n",
    "            gray_rate = np.mean((predictions >= low) & (predictions <= high))\n",
    "            if abs(gray_rate - target) < abs(best_rate - target):\n",
    "                best_low, best_high, best_rate = low, high, gray_rate\n",
    "\n",
    "    return best_low, best_high, best_rate\n",
    "\n",
    "\n",
    "# Assume we have the optimal model and threshold from previous analysis\n",
    "print(\"THRESHOLD OPTIMIZATION RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Optimal threshold for maximum F1\n",
    "print(f\"\\n1. Optimal decision threshold:\")\n",
    "print(f\"  t_star: {t_star:.3f}\")\n",
    "\n",
    "# Calculate F1 at optimal threshold\n",
    "y_pred_optimal = (p_mal > t_star).astype(int)\n",
    "f1_optimal = f1_score(y_val, y_pred_optimal, average=\"macro\")\n",
    "print(f\"  F1-macro @t_star: {f1_optimal:.4f}\")\n",
    "\n",
    "# 2. Gray-zone band for judge integration\n",
    "print(\"\\n2. Standard gray-zone band (target: 10-15%):\")\n",
    "low, high, gray_rate = pick_band_for_target(p_mal, t_star, target=0.12)\n",
    "\n",
    "print(f\"  Low threshold: {low:.3f}\")\n",
    "print(f\"  High threshold: {high:.3f}\")\n",
    "print(f\"  Gray-zone rate: {gray_rate:.1%}\")\n",
    "\n",
    "# Standard decision categories\n",
    "decisions = pd.cut(\n",
    "    p_mal,\n",
    "    bins=[0, low, high, 1.0],\n",
    "    labels=[\"ALLOW\", \"REVIEW\", \"BLOCK\"],\n",
    "    include_lowest=True,\n",
    ")\n",
    "\n",
    "print(\"\\n3. Standard decision distribution:\")\n",
    "# Fix: Calculate proportions manually instead of using normalize=True\n",
    "counts = decisions.value_counts().sort_index()\n",
    "proportions = counts / len(decisions)\n",
    "for category, prop in proportions.items():\n",
    "    print(f\"  {category}: {prop:.1%} ({counts[category]:,} samples)\")\n",
    "\n",
    "\n",
    "# Enhanced decision logic with judge integration\n",
    "def enhanced_decision_logic(url, ml_confidence, low_thresh, high_thresh):\n",
    "    \"\"\"\n",
    "    Enhanced decision logic with judge integration for edge cases.\n",
    "\n",
    "    Parameters:\n",
    "    - url: The URL being evaluated\n",
    "    - ml_confidence: ML model confidence (p_malicious)\n",
    "    - low_thresh, high_thresh: Gray-zone boundaries\n",
    "\n",
    "    Returns:\n",
    "    - decision: \"ALLOW\", \"REVIEW\", or \"BLOCK\"\n",
    "    - reasoning: Explanation of the decision\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        domain = urlparse(url).netloc.lower()  # Extract domain portion of URL\n",
    "    except:\n",
    "        domain = \"\"\n",
    "\n",
    "    # High confidence cases\n",
    "    if ml_confidence < low_thresh:\n",
    "        return \"ALLOW\", f\"High confidence legitimate (p={ml_confidence:.3f})\"\n",
    "    elif ml_confidence > high_thresh:\n",
    "        return \"BLOCK\", f\"High confidence malicious (p={ml_confidence:.3f})\"\n",
    "\n",
    "    # Gray-zone: Check for judge criteria\n",
    "    else:\n",
    "        # Known legitimate short domains that might confuse the model\n",
    "        well_known_domains = {\n",
    "            \"google.com\",\n",
    "            \"github.com\",\n",
    "            \"microsoft.com\",\n",
    "            \"amazon.com\",\n",
    "            \"apple.com\",\n",
    "            \"facebook.com\",\n",
    "            \"twitter.com\",\n",
    "            \"linkedin.com\",\n",
    "            \"youtube.com\",\n",
    "            \"wikipedia.org\",\n",
    "            \"stackoverflow.com\",\n",
    "        }\n",
    "\n",
    "        if domain in well_known_domains:\n",
    "            return \"ALLOW\", f\"Well-known domain override (p={ml_confidence:.3f})\"\n",
    "        elif len(domain) <= 10 and ml_confidence < 0.5:\n",
    "            return (\n",
    "                \"REVIEW\",\n",
    "                f\"Short domain, moderate confidence (p={ml_confidence:.3f})\",\n",
    "            )\n",
    "        else:\n",
    "            return \"REVIEW\", f\"Uncertain classification (p={ml_confidence:.3f})\"\n",
    "\n",
    "\n",
    "# Test enhanced decision logic on sample cases\n",
    "print(\"\\n4. Enhanced Decision Logic Examples:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "test_cases = [\n",
    "    (\"https://images.google.com\", 0.25),\n",
    "    (\"https://github.com/user/repo\", 0.35),\n",
    "    (\"https://suspicious-bank-login.com\", 0.45),\n",
    "    (\"https://definitely-phishing-site.evil\", 0.85),\n",
    "    (\"https://legitimate-company.com\", 0.15),\n",
    "]\n",
    "\n",
    "for url, confidence in test_cases:\n",
    "    decision, reasoning = enhanced_decision_logic(url, confidence, low, high)\n",
    "    print(f\"  URL: {url[:40]:<40} | Decision: {decision:<6} | {reasoning}\")\n",
    "\n",
    "# Save thresholds for production use\n",
    "threshold_config = {\n",
    "    \"optimal_threshold\": float(t_star),\n",
    "    \"gray_zone_low\": float(low),\n",
    "    \"gray_zone_high\": float(high),\n",
    "    \"gray_zone_rate\": float(gray_rate),\n",
    "    \"f1_score_at_optimal\": float(f1_optimal),\n",
    "    \"decision_distribution\": {\n",
    "        \"allow_rate\": float(proportions.get(\"ALLOW\", 0)),\n",
    "        \"review_rate\": float(proportions.get(\"REVIEW\", 0)),\n",
    "        \"block_rate\": float(proportions.get(\"BLOCK\", 0)),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"\\n5. Configuration saved to: {THRESH_PATH}\")\n",
    "with open(THRESH_PATH, \"w\") as f:\n",
    "    json.dump(threshold_config, f, indent=2)\n",
    "\n",
    "print(\"\\nThreshold optimization complete! ‚úì\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c644030",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **SECTION 5: SPOT CHECK & Model Performance Evaluation**\n",
    "\n",
    "**Purpose:** Evaluate trained models to identify the best performer and validate training quality.\n",
    "\n",
    "**Workflow:**\n",
    "1. **Model Selection** - Compare performance metrics across candidates\n",
    "2. **Training Quality Assessment** - Validate model reliability and detect potential issues\n",
    "3. **URLS MISCLASSIFIED AS PHISHING** - Why URLs are being missclassified\n",
    "\n",
    "**Key Deliverables:**\n",
    "- Best performing model identification\n",
    "- Model validation report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cb4f9c",
   "metadata": {},
   "source": [
    "##### **5.1 How many samples predict as 1.0 in validation?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1c19832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation prediction distribution:\n",
      "  Extreme phishing (p >= 0.99): 19,490 (41.5%)\n",
      "  Moderate (0.01 < p < 0.99): 1,557 (3.3%)\n",
      "  Extreme legit (p <= 0.01): 25,906 (55.2%)\n",
      "\n",
      "Sample p_malicious: [0.00815403 1.         0.00186653 0.00126379 0.00186653]\n"
     ]
    }
   ],
   "source": [
    "# Count extreme predictions\n",
    "extreme_phish = (p_mal >= 0.99).sum()\n",
    "extreme_legit = (p_mal <= 0.01).sum()\n",
    "moderate = ((p_mal > 0.01) & (p_mal < 0.99)).sum()\n",
    "\n",
    "print(f\"\\nValidation prediction distribution:\")\n",
    "print(\n",
    "    f\"  Extreme phishing (p >= 0.99): {extreme_phish:,} ({extreme_phish / len(p_mal):.1%})\"\n",
    ")\n",
    "print(f\"  Moderate (0.01 < p < 0.99): {moderate:,} ({moderate / len(p_mal):.1%})\")\n",
    "print(\n",
    "    f\"  Extreme legit (p <= 0.01): {extreme_legit:,} ({extreme_legit / len(p_mal):.1%})\"\n",
    ")\n",
    "print(f\"\\nSample p_malicious: {p_mal[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b0b14c",
   "metadata": {},
   "source": [
    "##### **5.2 IDENTIFY URLS MISCLASSIFIED AS PHISHING**\n",
    "- Purpose:\n",
    "    - Identify short legitimate URLs that are being incorrectly classified as phishing.\n",
    "    - This helps understand model biases and potential issues with short domain classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "823a2bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SPOT CHECK: URLs MISCLASSIFIED AS PHISHING\n",
      "============================================================\n",
      "Total legitimate samples in validation: 26970\n",
      "Legitimate samples misclassified as phishing: 23\n",
      "Misclassification rate: 0.09%\n",
      "\n",
      "üîç TOP SHORT LEGITIMATE URLS MISCLASSIFIED AS PHISHING:\n",
      "================================================================================\n",
      "URL                                 Domain               P(phish)   HTTPS  TLD_Prob Dom_Len\n",
      "--------------------------------------------------------------------------------\n",
      "https://www.it120.cc                www.it120.cc         0.781      1      0.159    12     \n",
      "\n",
      "üìä ANALYSIS OF MISCLASSIFIED SHORT URLS:\n",
      "--------------------------------------------------\n",
      "  Average IsHTTPS: 1.00 (0=HTTP, 1=HTTPS)\n",
      "  Average TLD Legitimacy Prob: 0.159\n",
      "  Average Character Continuation Rate: 0.263\n",
      "  Average Special Character Ratio: 0.250\n",
      "  Domain lengths: min=12, max=12, avg=12.0\n",
      "\n",
      "  Most frequently misclassified short domains:\n",
      "    www.it120.cc: 1 times\n",
      "\n",
      "üî¨ COMPARISON WITH TRAINING DATA DISTRIBUTION:\n",
      "--------------------------------------------------\n",
      "  TLDLegitimateProb:\n",
      "    Misclassified short URLs: 0.159\n",
      "    Training legitimate URLs: 0.709\n",
      "    Difference: -0.551\n",
      "  CharContinuationRate:\n",
      "    Misclassified short URLs: 0.263\n",
      "    Training legitimate URLs: 0.169\n",
      "    Difference: 0.094\n",
      "  SpacialCharRatioInURL:\n",
      "    Misclassified short URLs: 0.250\n",
      "    Training legitimate URLs: 0.198\n",
      "    Difference: 0.052\n",
      "  DomainLength:\n",
      "    Misclassified short URLs: 12.000\n",
      "    Training legitimate URLs: 19.215\n",
      "    Difference: -7.215\n",
      "\n",
      "============================================================\n",
      "SPECIFIC EXAMPLES: WELL-KNOWN DOMAINS\n",
      "============================================================\n",
      "URL                       P(phishing)  Classification  Status    \n",
      "--------------------------------------------------------------\n",
      "https://google.com        1.000        PHISHING        ‚ùå WRONG   \n",
      "https://github.com        1.000        PHISHING        ‚ùå WRONG   \n",
      "https://fb.com            1.000        PHISHING        ‚ö†Ô∏è CHECK  \n",
      "https://bit.ly            1.000        PHISHING        ‚ö†Ô∏è CHECK  \n",
      "https://t.co              0.943        PHISHING        ‚ö†Ô∏è CHECK  \n",
      "https://apple.com         1.000        PHISHING        ‚ùå WRONG   \n",
      "https://amazon.com        1.000        PHISHING        ‚ùå WRONG   \n",
      "https://microsoft.com     1.000        PHISHING        ‚ùå WRONG   \n",
      "\n",
      "üí° INSIGHTS:\n",
      "- Look for patterns in misclassified short domains\n",
      "- Check if TLD legitimacy probabilities are unusually low\n",
      "- Verify if domain length feature is causing bias against short domains\n",
      "- Consider adding domain whitelist for well-known legitimate short domains\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# SPOT CHECK:  URLS MISCLASSIFIED AS PHISHING\n",
    "# ========================================\n",
    "\"\"\"\n",
    "**Purpose:** Identify legitimate URLs that are being incorrectly classified as phishing.\n",
    "This helps understand model biases and potential issues with domain classification.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SPOT CHECK: URLs MISCLASSIFIED AS PHISHING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the full dataset to get URLs\n",
    "df_full = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Get legitimate URLs from validation set that are predicted as phishing\n",
    "val_indices = X_val.index\n",
    "legitimate_val_indices = val_indices[y_val == 1]  # Legitimate samples in validation\n",
    "misclassified_mask = p_mal >= 0.5  # Predicted as phishing (high p_malicious)\n",
    "\n",
    "# Find legitimate URLs predicted as phishing\n",
    "misclassified_legit_indices = legitimate_val_indices[misclassified_mask[y_val == 1]]\n",
    "\n",
    "print(f\"Total legitimate samples in validation: {len(legitimate_val_indices)}\")\n",
    "print(\n",
    "    f\"Legitimate samples misclassified as phishing: {len(misclassified_legit_indices)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Misclassification rate: {len(misclassified_legit_indices) / len(legitimate_val_indices):.2%}\"\n",
    ")\n",
    "\n",
    "if len(misclassified_legit_indices) > 0:\n",
    "    # Get the URLs for misclassified samples\n",
    "    misclassified_urls = df_full.loc[misclassified_legit_indices, \"URL\"]\n",
    "    misclassified_probabilities = p_mal[y_val == 1][misclassified_mask[y_val == 1]]\n",
    "\n",
    "    # Focus on short URLs (domain length <= 15 characters)\n",
    "    short_url_data = []\n",
    "\n",
    "    for idx, (url, prob) in enumerate(\n",
    "        zip(misclassified_urls, misclassified_probabilities)\n",
    "    ):\n",
    "        try:\n",
    "            domain = urlparse(url).netloc.lower()\n",
    "            domain_length = len(domain)\n",
    "\n",
    "            if domain_length <= 15:  # Short domains\n",
    "                # Get the features for this URL\n",
    "                features = df_full.loc[misclassified_urls.index[idx], OPTIMAL_FEATURES]\n",
    "\n",
    "                short_url_data.append(\n",
    "                    {\n",
    "                        \"url\": url,\n",
    "                        \"domain\": domain,\n",
    "                        \"domain_length\": domain_length,\n",
    "                        \"p_malicious\": prob,\n",
    "                        \"IsHTTPS\": features[\"IsHTTPS\"],\n",
    "                        \"TLDLegitimateProb\": features[\"TLDLegitimateProb\"],\n",
    "                        \"CharContinuationRate\": features[\"CharContinuationRate\"],\n",
    "                        \"SpacialCharRatioInURL\": features[\"SpacialCharRatioInURL\"],\n",
    "                        \"URLCharProb\": features[\"URLCharProb\"],\n",
    "                        \"LetterRatioInURL\": features[\"LetterRatioInURL\"],\n",
    "                        \"NoOfOtherSpecialCharsInURL\": features[\n",
    "                            \"NoOfOtherSpecialCharsInURL\"\n",
    "                        ],\n",
    "                        \"DomainLength\": features[\"DomainLength\"],\n",
    "                    }\n",
    "                )\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # Sort by probability (highest misclassification confidence first)\n",
    "    short_url_data = sorted(\n",
    "        short_url_data, key=lambda x: x[\"p_malicious\"], reverse=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüîç TOP SHORT LEGITIMATE URLS MISCLASSIFIED AS PHISHING:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        f\"{'URL':<35} {'Domain':<20} {'P(phish)':<10} {'HTTPS':<6} {'TLD_Prob':<8} {'Dom_Len':<7}\"\n",
    "    )\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Show top 15 misclassified short URLs\n",
    "    for item in short_url_data[:15]:\n",
    "        url_short = (\n",
    "            item[\"url\"][:34] if len(item[\"url\"]) <= 34 else item[\"url\"][:31] + \"...\"\n",
    "        )\n",
    "        domain_short = (\n",
    "            item[\"domain\"][:19]\n",
    "            if len(item[\"domain\"]) <= 19\n",
    "            else item[\"domain\"][:16] + \"...\"\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"{url_short:<35} {domain_short:<20} {item['p_malicious']:<10.3f} {item['IsHTTPS']:<6.0f} {item['TLDLegitimateProb']:<8.3f} {item['DomainLength']:<7.0f}\"\n",
    "        )\n",
    "\n",
    "    # Analyze patterns in misclassified short URLs\n",
    "    print(f\"\\nüìä ANALYSIS OF MISCLASSIFIED SHORT URLS:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    if short_url_data:\n",
    "        # Common characteristics\n",
    "        avg_https = np.mean([item[\"IsHTTPS\"] for item in short_url_data])\n",
    "        avg_tld_prob = np.mean([item[\"TLDLegitimateProb\"] for item in short_url_data])\n",
    "        avg_char_cont = np.mean(\n",
    "            [item[\"CharContinuationRate\"] for item in short_url_data]\n",
    "        )\n",
    "        avg_special_ratio = np.mean(\n",
    "            [item[\"SpacialCharRatioInURL\"] for item in short_url_data]\n",
    "        )\n",
    "\n",
    "        print(f\"  Average IsHTTPS: {avg_https:.2f} (0=HTTP, 1=HTTPS)\")\n",
    "        print(f\"  Average TLD Legitimacy Prob: {avg_tld_prob:.3f}\")\n",
    "        print(f\"  Average Character Continuation Rate: {avg_char_cont:.3f}\")\n",
    "        print(f\"  Average Special Character Ratio: {avg_special_ratio:.3f}\")\n",
    "\n",
    "        # Domain length distribution\n",
    "        domain_lengths = [item[\"domain_length\"] for item in short_url_data]\n",
    "        print(\n",
    "            f\"  Domain lengths: min={min(domain_lengths)}, max={max(domain_lengths)}, avg={np.mean(domain_lengths):.1f}\"\n",
    "        )\n",
    "\n",
    "        # Common domains\n",
    "        domains = [item[\"domain\"] for item in short_url_data]\n",
    "        domain_counts = pd.Series(domains).value_counts()\n",
    "\n",
    "        print(f\"\\n  Most frequently misclassified short domains:\")\n",
    "        for domain, count in domain_counts.head(10).items():\n",
    "            print(f\"    {domain}: {count} times\")\n",
    "\n",
    "        # Feature comparison with training data\n",
    "        print(f\"\\nüî¨ COMPARISON WITH TRAINING DATA DISTRIBUTION:\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        legit_train = X_train[y_train == 1]  # Legitimate training samples\n",
    "\n",
    "        for feature in [\n",
    "            \"TLDLegitimateProb\",\n",
    "            \"CharContinuationRate\",\n",
    "            \"SpacialCharRatioInURL\",\n",
    "            \"DomainLength\",\n",
    "        ]:\n",
    "            misclass_values = [item[feature] for item in short_url_data]\n",
    "            train_values = legit_train[feature].values\n",
    "\n",
    "            misclass_mean = np.mean(misclass_values)\n",
    "            train_mean = train_values.mean()\n",
    "\n",
    "            print(f\"  {feature}:\")\n",
    "            print(f\"    Misclassified short URLs: {misclass_mean:.3f}\")\n",
    "            print(f\"    Training legitimate URLs: {train_mean:.3f}\")\n",
    "            print(f\"    Difference: {misclass_mean - train_mean:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚úÖ No legitimate URLs misclassified as phishing!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SPECIFIC EXAMPLES: WELL-KNOWN DOMAINS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test specific well-known short domains\n",
    "test_domains = [\n",
    "    \"https://google.com\",\n",
    "    \"https://github.com\",\n",
    "    \"https://fb.com\",\n",
    "    \"https://bit.ly\",\n",
    "    \"https://t.co\",\n",
    "    \"https://apple.com\",\n",
    "    \"https://amazon.com\",\n",
    "    \"https://microsoft.com\",\n",
    "]\n",
    "\n",
    "print(f\"{'URL':<25} {'P(phishing)':<12} {'Classification':<15} {'Status':<10}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for url in test_domains:\n",
    "    try:\n",
    "        # Extract features for this URL\n",
    "        features = extract_features(url, include_https=True)\n",
    "        feature_array = np.array([features[feat] for feat in OPTIMAL_FEATURES]).reshape(\n",
    "            1, -1\n",
    "        )\n",
    "\n",
    "        # Get probability\n",
    "        p_phish = best_model.predict_proba(feature_array)[0, 0]\n",
    "\n",
    "        # Classify\n",
    "        if p_phish >= 0.5:\n",
    "            classification = \"PHISHING\"\n",
    "            status = (\n",
    "                \"‚ùå WRONG\"\n",
    "                if any(\n",
    "                    known in url.lower()\n",
    "                    for known in [\"google\", \"github\", \"apple\", \"amazon\", \"microsoft\"]\n",
    "                )\n",
    "                else \"‚ö†Ô∏è CHECK\"\n",
    "            )\n",
    "        else:\n",
    "            classification = \"LEGITIMATE\"\n",
    "            status = \"‚úÖ CORRECT\"\n",
    "\n",
    "        print(f\"{url:<25} {p_phish:<12.3f} {classification:<15} {status:<10}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{url:<25} {'ERROR':<12} {'FAILED':<15} {'‚ùå ERROR':<10}\")\n",
    "\n",
    "print(\"\\nüí° INSIGHTS:\")\n",
    "print(\"- Look for patterns in misclassified short domains\")\n",
    "print(\"- Check if TLD legitimacy probabilities are unusually low\")\n",
    "print(\"- Verify if domain length feature is causing bias against short domains\")\n",
    "print(\"- Consider adding domain whitelist for well-known legitimate short domains\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb60aad",
   "metadata": {},
   "source": [
    "\n",
    "**The Key Insight**\n",
    "\n",
    "- **Looking at these critical facts:**\n",
    "\n",
    "    1. The Model's Performance is EXCELLENT\n",
    "\n",
    "        ```\n",
    "        Validation prediction distribution:\n",
    "        Extreme phishing (p >= 0.99): 19,490 (41.5%)  ‚Üê Confident phishing\n",
    "        Moderate (0.01 < p < 0.99): 1,557 (3.3%)      ‚Üê Uncertain\n",
    "        Extreme legit (p <= 0.01): 25,906 (55.2%)     ‚Üê Confident legit\n",
    "        ```\n",
    "        - 96.7% of predictions are confident! Only 3.3% are uncertain\n",
    "\n",
    "    2. Misclassification Rate is TINY\n",
    "    \n",
    "        ```\n",
    "        Total legitimate samples in validation: 26,970\n",
    "        Legitimate samples misclassified as phishing: 23\n",
    "        Misclassification rate: 0.09%\n",
    "        ```\n",
    "        - Only 23 out of 26,970 legitimate URLs are misclassified!\n",
    "        - That's 99.91% accuracy on legitimate URLs!\n",
    "\n",
    "    3. But Why Do google.com, github.com, etc. Get 1.0?\n",
    "        -Because they're NOT in the training data!\n",
    "        - Your training data is from PhiUSIIL dataset which:\n",
    "\n",
    "        - Focused on obscure/suspicious URLs\n",
    "        - Didn't include major tech companies\n",
    "        - Used URLs from 2019-2020 era\n",
    "\n",
    "        - google.com, github.com, amazon.com are OUT-OF-DISTRIBUTION for this model!\n",
    "\n",
    "**The Real Issue: Distribution Shift**\n",
    "\n",
    "***Training Data Characteristics***   \n",
    "    \n",
    "    ```\n",
    "    Training legitimate URLs:\n",
    "    Average TLDLegitimateProb: 0.709\n",
    "    Average DomainLength: 19.2 characters\n",
    "    TLDs: Mostly .com, .org, .net, .edu from dataset\n",
    "    ```\n",
    "***google.com Characteristics***\n",
    "\n",
    "    ```\n",
    "    google.com:\n",
    "    TLDLegitimateProb: 0.6111  ‚Üê Lower than training average!\n",
    "    DomainLength: 10           ‚Üê Much shorter than training average!\n",
    "    Pattern: Very short, very simple ‚Üí looks \"suspicious\" to model\n",
    "    ```\n",
    "**Why? Because:**\n",
    "\n",
    "- Training data has longer domains (avg 19 chars)\n",
    "- Training data has higher TLD probs (avg 0.71)\n",
    "- google.com is shorter (10 chars) with lower TLD prob (0.61)\n",
    "- To the model: \"This domain is too short and has an unusual TLD probability ‚Üí probably phishing!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9ab19",
   "metadata": {},
   "source": [
    "#### **5.3 Model Validation & Quality Assurance**\n",
    "\n",
    "**Purpose:** Validate model performance and ensure training quality through comprehensive checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40bd7eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç VALIDATING PROBABILITY FIX\n",
      "============================================================\n",
      "\n",
      "üìä TESTING FIXED MODEL ON KNOWN URLS:\n",
      "------------------------------------------------------------\n",
      "URL                                      p_malicious  Assessment     \n",
      "------------------------------------------------------------\n",
      "https://google.com                       1.000        LOW (legit)     ‚ùå BAD\n",
      "https://images.google.com                0.999        LOW (legit)     ‚ùå BAD\n",
      "https://facebook.com                     1.000        LOW (legit)     ‚ùå BAD\n",
      "https://github.com                       1.000        LOW (legit)     ‚ùå BAD\n",
      "http://suspicious-phishing-site.tk       1.000        HIGH (phish)    ‚úÖ GOOD\n",
      "https://love.ai/chat/3263869b-a416-4550-9d3d-f1d0f951c42a.com 1.000        HIGH (phish)    ‚úÖ GOOD\n",
      "\n",
      "üéØ PROBABILITY DISTRIBUTION CHECK:\n",
      "  Min p_malicious: 0.000\n",
      "  Max p_malicious: 1.000\n",
      "  Mean p_malicious: 0.425\n",
      "  Std p_malicious: 0.492\n",
      "  ‚úÖ GOOD: Full probability range [0,1] utilized\n",
      "\n",
      "============================================================\n",
      "COMPREHENSIVE DIAGNOSIS - FINDING THE ROOT CAUSE\n",
      "================================================================================\n",
      "HYPOTHESIS 1: Testing on actual training samples\n",
      "--------------------------------------------------\n",
      "Testing on TRAINING samples (should predict correctly):\n",
      "  Legit sample 1: P(phishing)=0.002325, Actual=1\n",
      "  Legit sample 2: P(phishing)=0.003466, Actual=1\n",
      "  Legit sample 3: P(phishing)=0.001238, Actual=1\n",
      "  Phish sample 1: P(phishing)=1.000000, Actual=0\n",
      "  Phish sample 2: P(phishing)=1.000000, Actual=0\n",
      "  Phish sample 3: P(phishing)=1.000000, Actual=0\n",
      "--------------------------------------------------\n",
      "\n",
      "HYPOTHESIS 2: Feature extraction mismatch\n",
      "--------------------------------------------------\n",
      "Sample URL: https://www.southbankmosaics.com\n",
      "Comparing features extracted vs. training data:\n",
      "  IsHTTPS                       : Data=1.000000 | Extracted=1.000000 | MATCH\n",
      "  TLDLegitimateProb             : Data=0.612000 | Extracted=0.611997 | MATCH\n",
      "  CharContinuationRate          : Data=0.129032 | Extracted=0.129032 | MATCH\n",
      "  SpacialCharRatioInURL         : Data=0.156250 | Extracted=0.156250 | MATCH\n",
      "  URLCharProb                   : Data=0.060000 | Extracted=1.000000 | DIFF=0.940000\n",
      "  LetterRatioInURL              : Data=0.843750 | Extracted=0.843750 | MATCH\n",
      "  NoOfOtherSpecialCharsInURL    : Data=5.000000 | Extracted=5.000000 | MATCH\n",
      "  DomainLength                  : Data=24.000000 | Extracted=24.000000 | MATCH\n",
      "--------------------------------------------------\n",
      "\n",
      "HYPOTHESIS 3: Model calibration/threshold issue\n",
      "--------------------------------------------------\n",
      "Testing probability extraction on validation samples:\n",
      "  Legitimate sample: [0.00815403 0.99184597] | P(phish)=0.008154\n",
      "  Phishing sample:   [1. 0.] | P(phish)=1.000000\n",
      "============================================================\n",
      "FINDING LEGITIMATE URLs THAT MODEL HANDLES CORRECTLY:\n",
      "============================================================\n",
      "Testing 10 legitimate URLs from training data:\n",
      "------------------------------------------------------------\n",
      "URL                                                P(phish)   Status  \n",
      "------------------------------------------------------------\n",
      "https://www.atelierozmoz.be                        0.000      GOOD    \n",
      "https://www.diemon.com                             0.002      GOOD    \n",
      "https://www.wausauschools.org                      0.001      GOOD    \n",
      "https://www.paademode.com                          0.002      GOOD    \n",
      "https://www.boxturtles.com                         0.002      GOOD    \n",
      "https://www.mmstadium.com                          0.002      GOOD    \n",
      "https://www.brswimwear.com                         0.002      GOOD    \n",
      "https://www.leathercouncil.org                     0.001      GOOD    \n",
      "https://www.historync.org                          0.002      GOOD    \n",
      "https://www.toshin.com                             0.002      GOOD    \n",
      "\n",
      "CONCLUSION:\n",
      "If training URLs work but google.com doesn't, it confirms\n",
      "that google.com has features outside the training distribution.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç VALIDATING PROBABILITY FIX\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test the best model on some known URLs to verify fix\n",
    "test_validation_urls = [\n",
    "    \"https://google.com\",\n",
    "    \"https://images.google.com\",\n",
    "    \"https://facebook.com\",\n",
    "    \"https://github.com\",\n",
    "    \"http://suspicious-phishing-site.tk\",\n",
    "    \"https://love.ai/chat/3263869b-a416-4550-9d3d-f1d0f951c42a.com\",\n",
    "]\n",
    "\n",
    "\n",
    "print(\"\\nüìä TESTING FIXED MODEL ON KNOWN URLS:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'URL':<40} {'p_malicious':<12} {'Assessment':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for url in test_validation_urls:\n",
    "    # Extract features\n",
    "    features = extract_features(url, include_https=True)\n",
    "    feature_array = np.array([features[feat] for feat in OPTIMAL_FEATURES]).reshape(\n",
    "        1, -1\n",
    "    )\n",
    "\n",
    "    # Get probability using fixed model\n",
    "    p_mal_pred = best_model.predict_proba(feature_array)[0, 0]  # Should be P(phishing)\n",
    "\n",
    "    # Assess if prediction makes sense\n",
    "    if \"google\" in url or \"facebook\" in url or \"github\" in url:\n",
    "        expected = \"LOW (legit)\"\n",
    "        reasonable = p_mal_pred < 0.5\n",
    "    else:\n",
    "        expected = \"HIGH (phish)\"\n",
    "        reasonable = p_mal_pred > 0.5\n",
    "\n",
    "    status = \"‚úÖ GOOD\" if reasonable else \"‚ùå BAD\"\n",
    "\n",
    "    print(f\"{url:<40} {p_mal_pred:<12.3f} {expected:<15} {status}\")\n",
    "\n",
    "print(\"\\nüéØ PROBABILITY DISTRIBUTION CHECK:\")\n",
    "print(f\"  Min p_malicious: {p_mal.min():.3f}\")\n",
    "print(f\"  Max p_malicious: {p_mal.max():.3f}\")\n",
    "print(f\"  Mean p_malicious: {p_mal.mean():.3f}\")\n",
    "print(f\"  Std p_malicious: {p_mal.std():.3f}\")\n",
    "\n",
    "# Check if we're getting realistic probability range\n",
    "if p_mal.min() < 0.01 and p_mal.max() > 0.99:\n",
    "    print(\"  ‚úÖ GOOD: Full probability range [0,1] utilized\")\n",
    "elif p_mal.std() < 0.1:\n",
    "    print(\"  ‚ùå BAD: Very low probability variance - model may be broken\")\n",
    "    print(\"       All predictions too similar!\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  PARTIAL: Some probability spread, but check edge cases\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPREHENSIVE DIAGNOSIS - FINDING THE ROOT CAUSE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# HYPOTHESIS 1: Test on actual training samples (should work perfectly)\n",
    "print(\"HYPOTHESIS 1: Testing on actual training samples\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get a few legitimate samples from training data\n",
    "legit_indices = np.where(y_train == 1)[0][:3]  # First 3 legitimate samples\n",
    "phish_indices = np.where(y_train == 0)[0][:3]  # First 3 phishing samples\n",
    "\n",
    "print(\"Testing on TRAINING samples (should predict correctly):\")\n",
    "for i, idx in enumerate(legit_indices):\n",
    "    sample_features = X_train.iloc[idx].values.reshape(1, -1)\n",
    "    proba = best_model.predict_proba(sample_features)[0]\n",
    "    p_phish = proba[0]  # P(phishing)\n",
    "    actual_label = y_train[idx]\n",
    "\n",
    "    print(f\"  Legit sample {i + 1}: P(phishing)={p_phish:.6f}, Actual={actual_label}\")\n",
    "\n",
    "for i, idx in enumerate(phish_indices):\n",
    "    sample_features = X_train.iloc[idx].values.reshape(1, -1)\n",
    "    proba = best_model.predict_proba(sample_features)[0]\n",
    "    p_phish = proba[0]  # P(phishing)\n",
    "    actual_label = y_train[idx]\n",
    "\n",
    "    print(f\"  Phish sample {i + 1}: P(phishing)={p_phish:.6f}, Actual={actual_label}\")\n",
    "print(\"-\" * 50)\n",
    "# HYPOTHESIS 2: Compare feature extraction vs. training data\n",
    "print(\"\\nHYPOTHESIS 2: Feature extraction mismatch\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load original data to get a real URL and compare features\n",
    "df_orig = pd.read_csv(DATA_PATH)\n",
    "sample_url = df_orig[df_orig[\"label\"] == 1].iloc[0][\"URL\"]  # First legitimate URL\n",
    "actual_features_from_data = df_orig[df_orig[\"URL\"] == sample_url][\n",
    "    OPTIMAL_FEATURES\n",
    "].iloc[0]\n",
    "\n",
    "print(f\"Sample URL: {sample_url}\")\n",
    "print(\"Comparing features extracted vs. training data:\")\n",
    "\n",
    "# Extract features using our extraction function\n",
    "extracted_features = extract_features(sample_url, include_https=True)\n",
    "\n",
    "for feat in OPTIMAL_FEATURES:\n",
    "    data_val = actual_features_from_data[feat]\n",
    "    extracted_val = extracted_features[feat]\n",
    "    diff = abs(data_val - extracted_val)\n",
    "\n",
    "    match = \"MATCH\" if diff < 0.001 else f\"DIFF={diff:.6f}\"\n",
    "    print(\n",
    "        f\"  {feat:30s}: Data={data_val:8.6f} | Extracted={extracted_val:8.6f} | {match}\"\n",
    "    )\n",
    "\n",
    "print(\"-\" * 50)\n",
    "# HYPOTHESIS 3: Model calibration issue\n",
    "print(\"\\nHYPOTHESIS 3: Model calibration/threshold issue\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test if we're using the right probability column\n",
    "sample_legitimate_features = X_val[y_val == 1].iloc[0].values.reshape(1, -1)\n",
    "sample_phishing_features = X_val[y_val == 0].iloc[0].values.reshape(1, -1)\n",
    "\n",
    "print(\"Testing probability extraction on validation samples:\")\n",
    "proba_legit = best_model.predict_proba(sample_legitimate_features)[0]\n",
    "proba_phish = best_model.predict_proba(sample_phishing_features)[0]\n",
    "\n",
    "print(f\"  Legitimate sample: {proba_legit} | P(phish)={proba_legit[0]:.6f}\")\n",
    "print(f\"  Phishing sample:   {proba_phish} | P(phish)={proba_phish[0]:.6f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "# TEST: Find legitimate URLs from training data that model handles correctly\n",
    "print(\"FINDING LEGITIMATE URLs THAT MODEL HANDLES CORRECTLY:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the full data to get URLs\n",
    "df_full = pd.read_csv(DATA_PATH)\n",
    "legit_urls = df_full[df_full[\"label\"] == 1][\"URL\"].sample(10, random_state=42)\n",
    "\n",
    "print(\"Testing 10 legitimate URLs from training data:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'URL':<50} {'P(phish)':<10} {'Status':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for url in legit_urls:\n",
    "    # Extract features and predict\n",
    "    features = extract_features(url, include_https=True)\n",
    "    feature_array = np.array([features[feat] for feat in OPTIMAL_FEATURES]).reshape(\n",
    "        1, -1\n",
    "    )\n",
    "    p_phish = best_model.predict_proba(feature_array)[0, 0]\n",
    "\n",
    "    status = \"GOOD\" if p_phish < 0.3 else \"BAD\"\n",
    "    url_short = url[:49] if len(url) <= 49 else url[:46] + \"...\"\n",
    "\n",
    "    print(f\"{url_short:<50} {p_phish:<10.3f} {status:<8}\")\n",
    "\n",
    "print(\"\\nCONCLUSION:\")\n",
    "print(\"If training URLs work but google.com doesn't, it confirms\")\n",
    "print(\"that google.com has features outside the training distribution.\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a5e29",
   "metadata": {},
   "source": [
    "#### **5.4 Validation & QA**\n",
    "- Purpose: F1/PR-AUC/Brier checks, confusion sanity, outlier inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feeeb978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUGGING INDIVIDUAL URL PREDICTIONS:\n",
      "============================================================\n",
      "Testing URL: https://google.com\n",
      "Extracted features: ['IsHTTPS=1.000', 'TLDLegitimateProb=0.612', 'CharContinuationRate=0.176', 'SpacialCharRatioInURL=0.222', 'URLCharProb=1.000', 'LetterRatioInURL=0.778', 'NoOfOtherSpecialCharsInURL=4.000', 'DomainLength=10.000']\n",
      "Full prediction matrix: [[1. 0.]]\n",
      "Model classes: [0 1]\n",
      "Phishing index: 0, Legitimate index: 1\n",
      "P(phishing): 1.000000\n",
      "P(legitimate): 0.000000\n",
      "\n",
      "============================================================\n",
      "üìä COMPARING WITH TRAINING DATA:\n",
      "============================================================\n",
      "Google.com features:\n",
      "  IsHTTPS                       :    1.000 | Train Œº= 0.783 œÉ= 0.412 | Z=  0.53\n",
      "  TLDLegitimateProb             :    0.612 | Train Œº= 0.575 œÉ= 0.278 | Z=  0.13\n",
      "  CharContinuationRate          :    0.176 | Train Œº= 0.147 œÉ= 0.053 | Z=  0.56\n",
      "  SpacialCharRatioInURL         :    0.222 | Train Œº= 0.194 œÉ= 0.041 | Z=  0.68\n",
      "  URLCharProb                   :    1.000 | Train Œº= 0.060 œÉ= 0.000 | Z=4073.95\n",
      "    ‚ö†Ô∏è  OUTLIER: 4073.9 standard deviations from training data!\n",
      "  LetterRatioInURL              :    0.778 | Train Œº= 0.778 œÉ= 0.073 | Z= -0.00\n",
      "  NoOfOtherSpecialCharsInURL    :    4.000 | Train Œº= 6.187 œÉ= 4.497 | Z= -0.49\n",
      "  DomainLength                  :   10.000 | Train Œº=21.467 œÉ= 9.110 | Z= -1.26\n",
      "\n",
      "üéØ CHECKING LEGITIMATE SAMPLES IN TRAINING:\n",
      "Training legitimate samples: 107880\n",
      "Sample legitimate features (first 3 rows):\n",
      "        IsHTTPS  TLDLegitimateProb  CharContinuationRate  \\\n",
      "168150      1.0             0.6120              0.160000   \n",
      "214464      1.0             0.6947              0.190476   \n",
      "200072      1.0             0.8793              0.137931   \n",
      "\n",
      "        SpacialCharRatioInURL  URLCharProb  LetterRatioInURL  \\\n",
      "168150               0.192308         0.06          0.807692   \n",
      "214464               0.272727         0.06          0.727273   \n",
      "200072               0.166667         0.06          0.833333   \n",
      "\n",
      "        NoOfOtherSpecialCharsInURL  DomainLength  \n",
      "168150                           5            18  \n",
      "214464                           6            14  \n",
      "200072                           5            22  \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Check what's happening with individual predictions\n",
    "print(\"DEBUGGING INDIVIDUAL URL PREDICTIONS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "url = \"https://google.com\"\n",
    "features = extract_features(url, include_https=True)\n",
    "feature_array = np.array([features[feat] for feat in OPTIMAL_FEATURES]).reshape(1, -1)\n",
    "\n",
    "print(f\"Testing URL: {url}\")\n",
    "print(\n",
    "    f\"Extracted features: {[f'{feat}={features[feat]:.3f}' for feat in OPTIMAL_FEATURES]}\"\n",
    ")\n",
    "\n",
    "# Get full prediction matrix\n",
    "proba_matrix = best_model.predict_proba(feature_array)\n",
    "print(f\"Full prediction matrix: {proba_matrix}\")\n",
    "print(f\"Model classes: {best_model.classes_}\")\n",
    "\n",
    "# Check what index we're using\n",
    "phish_idx = list(best_model.classes_).index(0)\n",
    "legit_idx = list(best_model.classes_).index(1)\n",
    "print(f\"Phishing index: {phish_idx}, Legitimate index: {legit_idx}\")\n",
    "print(f\"P(phishing): {proba_matrix[0, phish_idx]:.6f}\")\n",
    "print(f\"P(legitimate): {proba_matrix[0, legit_idx]:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "# Compare with actual training data distribution\n",
    "print(\"üìä COMPARING WITH TRAINING DATA:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Google.com features:\")\n",
    "for feat in OPTIMAL_FEATURES:\n",
    "    google_val = features[feat]\n",
    "    train_col = X_train[feat]\n",
    "    train_mean = train_col.mean()\n",
    "    train_std = train_col.std()\n",
    "\n",
    "    # Z-score (how many std deviations away from training mean)\n",
    "    z_score = (google_val - train_mean) / train_std if train_std > 0 else 0\n",
    "\n",
    "    print(\n",
    "        f\"  {feat:30s}: {google_val:8.3f} | Train Œº={train_mean:6.3f} œÉ={train_std:6.3f} | Z={z_score:6.2f}\"\n",
    "    )\n",
    "\n",
    "    if abs(z_score) > 3:\n",
    "        print(\n",
    "            f\"    ‚ö†Ô∏è  OUTLIER: {abs(z_score):.1f} standard deviations from training data!\"\n",
    "        )\n",
    "\n",
    "print(\"\\nüéØ CHECKING LEGITIMATE SAMPLES IN TRAINING:\")\n",
    "legit_samples = X_train[y_train == 1]  # Legitimate samples\n",
    "print(f\"Training legitimate samples: {len(legit_samples)}\")\n",
    "print(f\"Sample legitimate features (first 3 rows):\")\n",
    "print(legit_samples.head(3))\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d64a2b",
   "metadata": {},
   "source": [
    "### **SECTION 6: Model Comparison & Ablation Analysis**\n",
    "\n",
    "**Objective:** Compare 7-feature vs 8-feature models to understand the impact of HTTPS feature on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdf76bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL COMPARISON: IsHTTPS IMPACT ANALYSIS\n",
      "============================================================\n",
      "\n",
      "MODEL 1: 8-Feature (with IsHTTPS)\n",
      "  Features: 8\n",
      "  PR-AUC: 0.9992\n",
      "  F1-macro: 0.9970\n",
      "  Brier Score: 0.0026\n",
      "  Optimal threshold: 0.350\n",
      "\n",
      "7-feature set (excluding IsHTTPS): ['TLDLegitimateProb', 'CharContinuationRate', 'SpacialCharRatioInURL', 'URLCharProb', 'LetterRatioInURL', 'NoOfOtherSpecialCharsInURL', 'DomainLength']\n",
      "\n",
      "Training 7-feature model...\n",
      "Training data shape: (187811, 7)\n",
      "Validation data shape: (46953, 7)\n",
      "Missing values in training: 0\n",
      "Missing values in validation: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MLops\\NetworkSecurity\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [11:11:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating 7-feature model...\n",
      "  7-feature model classes: [0 1]\n",
      "  Phishing prob column: 0, Legitimate prob column: 1\n",
      "  Sample p_mal: [0.01133018 1.         0.00214739] | Sample p_legit: [0.98866982 0.         0.99785261]\n",
      "\n",
      "7-Feature Model Performance:\n",
      "  PR-AUC: 0.9988\n",
      "  F1-macro @0.5: 0.9940\n",
      "  Brier Score: 0.0052\n",
      "\n",
      "Optimizing threshold for 7-feature model...\n",
      "  Optimal threshold: 0.470\n",
      "  F1-macro @optimal: 0.9940\n",
      "  Gray-zone low: 0.011\n",
      "  Gray-zone high: 0.998\n",
      "  Gray-zone rate: 12.0%\n",
      "\n",
      "MODEL 2: 7-Feature (without IsHTTPS)\n",
      "  Features: 7\n",
      "  PR-AUC: 0.9988\n",
      "  F1-macro: 0.9940\n",
      "  Brier Score: 0.0052\n",
      "  Optimal threshold: 0.470\n",
      "\n",
      "============================================================\n",
      "COMPARATIVE ANALYSIS: 8-FEAT vs 7-FEAT\n",
      "============================================================\n",
      "\n",
      "Model Comparison:\n",
      "                         Model  PR-AUC  F1-macro  Brier Score  \\\n",
      "0     8-feature (with IsHTTPS)  0.9992     0.997       0.0026   \n",
      "1  7-feature (without IsHTTPS)  0.9988     0.994       0.0052   \n",
      "\n",
      "   Optimal Threshold  Gray-zone Rate  \n",
      "0               0.35          0.1094  \n",
      "1               0.47          0.1199  \n",
      "\n",
      "Performance Impact of Removing IsHTTPS:\n",
      "  PR-AUC drop: 0.0004 (0.04%)\n",
      "  F1-macro drop: 0.0030 (0.30%)\n",
      "  Brier score increase: 0.0026 (97.20%)\n",
      "\n",
      "Feature Importance Analysis:\n",
      "\n",
      "8-Feature Model - Feature Importance:\n",
      "  IsHTTPS                       : 0.8619\n",
      "  CharContinuationRate          : 0.0712\n",
      "  NoOfOtherSpecialCharsInURL    : 0.0283\n",
      "  TLDLegitimateProb             : 0.0200\n",
      "  LetterRatioInURL              : 0.0117\n",
      "  DomainLength                  : 0.0040\n",
      "  SpacialCharRatioInURL         : 0.0030\n",
      "  URLCharProb                   : 0.0000\n",
      "\n",
      "IsHTTPS importance rank in 8-feature model: 1/8\n",
      "IsHTTPS importance value: 0.8619\n",
      "\n",
      "7-Feature Model - Feature Importance:\n",
      "  CharContinuationRate          : 0.2874\n",
      "  NoOfOtherSpecialCharsInURL    : 0.2722\n",
      "  TLDLegitimateProb             : 0.1767\n",
      "  LetterRatioInURL              : 0.0992\n",
      "  SpacialCharRatioInURL         : 0.0837\n",
      "  DomainLength                  : 0.0808\n",
      "  URLCharProb                   : 0.0000\n",
      "\n",
      "Conclusion:\n",
      "‚úì IsHTTPS has minimal impact on model performance\n",
      "‚úì 7-feature model is suitable for production (URL-only features)\n",
      "\n",
      "Recommendation: Use 7-feature model\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON: IsHTTPS IMPACT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================\n",
    "# MODEL 1: 8-FEATURE (WITH IsHTTPS) - Already trained\n",
    "# ============================================================\n",
    "\n",
    "model_8feat = {\n",
    "    \"name\": \"8-feature (with IsHTTPS)\",\n",
    "    \"features\": OPTIMAL_FEATURES,\n",
    "    \"model\": best_model,\n",
    "    \"p_malicious\": p_mal,\n",
    "    \"metrics\": {\n",
    "        \"pr_auc\": best_metrics[\"pr_auc_phish\"],\n",
    "        \"f1_macro\": best_metrics[\"f1_macro@0.5_on_p_mal\"],\n",
    "        \"brier\": best_metrics[\"brier_phish\"],\n",
    "    },\n",
    "    \"thresholds\": threshold_config,\n",
    "}\n",
    "\n",
    "print(\"\\nMODEL 1: 8-Feature (with IsHTTPS)\")\n",
    "print(f\"  Features: {len(model_8feat['features'])}\")\n",
    "print(f\"  PR-AUC: {model_8feat['metrics']['pr_auc']:.4f}\")\n",
    "print(f\"  F1-macro: {model_8feat['metrics']['f1_macro']:.4f}\")\n",
    "print(f\"  Brier Score: {model_8feat['metrics']['brier']:.4f}\")\n",
    "print(f\"  Optimal threshold: {model_8feat['thresholds']['optimal_threshold']:.3f}\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL 2: 7-FEATURE (WITHOUT IsHTTPS) - Need to train\n",
    "# ============================================================\n",
    "\n",
    "# Define 7-feature set (excluding IsHTTPS)\n",
    "FEATURES_7 = [f for f in OPTIMAL_FEATURES if f != \"IsHTTPS\"]\n",
    "\n",
    "print(f\"\\n7-feature set (excluding IsHTTPS): {FEATURES_7}\")\n",
    "\n",
    "# Train 7-feature model\n",
    "print(\"\\nTraining 7-feature model...\")\n",
    "\n",
    "# Prepare 7-feature data\n",
    "X_train_7feat = X_train[FEATURES_7].copy()\n",
    "X_val_7feat = X_val[FEATURES_7].copy()\n",
    "\n",
    "print(f\"Training data shape: {X_train_7feat.shape}\")\n",
    "print(f\"Validation data shape: {X_val_7feat.shape}\")\n",
    "\n",
    "# Check for any missing values\n",
    "print(f\"Missing values in training: {X_train_7feat.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in validation: {X_val_7feat.isnull().sum().sum()}\")\n",
    "\n",
    "# Train XGBoost model with same parameters as 8-feature model\n",
    "xgb_7feat = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=SEED,\n",
    "    n_jobs=0,\n",
    "    objective=\"binary:logistic\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "xgb_7feat.fit(X_train_7feat, y_train)\n",
    "\n",
    "# Calibrate the model\n",
    "print(\"Calibrating 7-feature model...\")\n",
    "calib_7feat = CalibratedClassifierCV(\n",
    "    xgb_7feat,\n",
    "    method=\"isotonic\",\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED),\n",
    "    n_jobs=-1,\n",
    ")\n",
    "calib_7feat.fit(X_train_7feat, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "# FIXED: Extract P(phishing) from correct column (class 0)\n",
    "proba_matrix_7feat = calib_7feat.predict_proba(X_val_7feat)\n",
    "classes_7feat = calib_7feat.classes_\n",
    "phish_idx_7feat = list(classes_7feat).index(0)  # Find index of phishing class (label=0)\n",
    "legit_idx_7feat = list(classes_7feat).index(\n",
    "    1\n",
    ")  # Find index of legitimate class (label=1)\n",
    "p_mal_7feat = proba_matrix_7feat[:, phish_idx_7feat]  # Extract P(phishing)\n",
    "p_legit_7feat = proba_matrix_7feat[:, legit_idx_7feat]  # Extract P(legitimate)\n",
    "\n",
    "# Validation: Ensure probabilities sum to 1.0 (sanity check)\n",
    "prob_sum_check_7feat = np.allclose(p_mal_7feat + p_legit_7feat, 1.0)\n",
    "if not prob_sum_check_7feat:\n",
    "    raise ValueError(\n",
    "        f\"7-feature probability columns don't sum to 1.0! Classes: {classes_7feat}\"\n",
    "    )\n",
    "\n",
    "# Debug info\n",
    "print(f\"  7-feature model classes: {classes_7feat}\")\n",
    "print(\n",
    "    f\"  Phishing prob column: {phish_idx_7feat}, Legitimate prob column: {legit_idx_7feat}\"\n",
    ")\n",
    "print(f\"  Sample p_mal: {p_mal_7feat[:3]} | Sample p_legit: {p_legit_7feat[:3]}\")\n",
    "\n",
    "y_pred_7feat = calib_7feat.predict(X_val_7feat)\n",
    "\n",
    "# Calculate metrics for 7-feature model\n",
    "\n",
    "# PR-AUC for phishing detection (convert y_val to binary: 0=phish, 1=legit -> 1=phish, 0=legit)\n",
    "y_val_binary_phish = (y_val == 0).astype(int)  # Convert to binary phishing labels\n",
    "precision_7feat, recall_7feat, _ = precision_recall_curve(\n",
    "    y_val_binary_phish, p_mal_7feat\n",
    ")\n",
    "pr_auc_7feat = auc(recall_7feat, precision_7feat)\n",
    "\n",
    "# F1-macro at 0.5 threshold (convert predictions to match y_val format)\n",
    "y_pred_7feat_binary = (p_mal_7feat >= 0.5).astype(int)  # 1 = predict phishing\n",
    "y_pred_7feat_labels = (\n",
    "    1 - y_pred_7feat_binary\n",
    ")  # Convert to label space (0=phish, 1=legit)\n",
    "f1_7feat = f1_score(y_val, y_pred_7feat_labels, average=\"macro\")\n",
    "\n",
    "# Brier score for phishing detection\n",
    "brier_7feat = brier_score_loss(y_val_binary_phish, p_mal_7feat)\n",
    "\n",
    "print(f\"\\n7-Feature Model Performance:\")\n",
    "print(f\"  PR-AUC: {pr_auc_7feat:.4f}\")\n",
    "print(f\"  F1-macro @0.5: {f1_7feat:.4f}\")\n",
    "print(f\"  Brier Score: {brier_7feat:.4f}\")\n",
    "\n",
    "# Optimize threshold for 7-feature model\n",
    "print(\"\\nOptimizing threshold for 7-feature model...\")\n",
    "\n",
    "# Find optimal threshold\n",
    "f1s_7feat = []\n",
    "thresholds_test = np.arange(0.1, 0.9, 0.01)\n",
    "\n",
    "for t in thresholds_test:\n",
    "    y_pred_t_binary = (p_mal_7feat >= t).astype(int)  # 1 = predict phishing\n",
    "    y_pred_t_labels = 1 - y_pred_t_binary  # Convert to label space (0=phish, 1=legit)\n",
    "    f1_t = f1_score(y_val, y_pred_t_labels, average=\"macro\")\n",
    "    f1s_7feat.append(f1_t)\n",
    "\n",
    "# Find best threshold\n",
    "best_idx_7feat = np.argmax(f1s_7feat)\n",
    "t_star_7feat = thresholds_test[best_idx_7feat]\n",
    "best_f1_7feat = f1s_7feat[best_idx_7feat]\n",
    "\n",
    "print(f\"  Optimal threshold: {t_star_7feat:.3f}\")\n",
    "print(f\"  F1-macro @optimal: {best_f1_7feat:.4f}\")\n",
    "\n",
    "# Create gray-zone thresholds for 7-feature model\n",
    "low_7feat, high_7feat, gray_7feat = pick_band_for_target(\n",
    "    p_mal_7feat, t_star_7feat, target=0.12\n",
    ")\n",
    "\n",
    "print(f\"  Gray-zone low: {low_7feat:.3f}\")\n",
    "print(f\"  Gray-zone high: {high_7feat:.3f}\")\n",
    "print(f\"  Gray-zone rate: {gray_7feat:.1%}\")\n",
    "\n",
    "# Create 7-feature model metadata\n",
    "threshold_config_7feat = {\n",
    "    \"optimal_threshold\": float(t_star_7feat),\n",
    "    \"gray_zone_low\": float(low_7feat),\n",
    "    \"gray_zone_high\": float(high_7feat),\n",
    "    \"gray_zone_rate\": float(gray_7feat),\n",
    "    \"f1_score_at_optimal\": float(best_f1_7feat),\n",
    "}\n",
    "\n",
    "model_7feat = {\n",
    "    \"name\": \"7-feature (without IsHTTPS)\",\n",
    "    \"features\": FEATURES_7,\n",
    "    \"model\": calib_7feat,\n",
    "    \"p_malicious\": p_mal_7feat,\n",
    "    \"metrics\": {\n",
    "        \"pr_auc\": pr_auc_7feat,\n",
    "        \"f1_macro\": f1_7feat,\n",
    "        \"brier\": brier_7feat,\n",
    "    },\n",
    "    \"thresholds\": threshold_config_7feat,\n",
    "}\n",
    "\n",
    "print(\"\\nMODEL 2: 7-Feature (without IsHTTPS)\")\n",
    "print(f\"  Features: {len(model_7feat['features'])}\")\n",
    "print(f\"  PR-AUC: {model_7feat['metrics']['pr_auc']:.4f}\")\n",
    "print(f\"  F1-macro: {model_7feat['metrics']['f1_macro']:.4f}\")\n",
    "print(f\"  Brier Score: {model_7feat['metrics']['brier']:.4f}\")\n",
    "print(f\"  Optimal threshold: {model_7feat['thresholds']['optimal_threshold']:.3f}\")\n",
    "\n",
    "# ============================================================\n",
    "# COMPARATIVE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARATIVE ANALYSIS: 8-FEAT vs 7-FEAT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison dataframe\n",
    "metrics_comp = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"8-feature (with IsHTTPS)\", \"7-feature (without IsHTTPS)\"],\n",
    "        \"PR-AUC\": [model_8feat[\"metrics\"][\"pr_auc\"], model_7feat[\"metrics\"][\"pr_auc\"]],\n",
    "        \"F1-macro\": [\n",
    "            model_8feat[\"metrics\"][\"f1_macro\"],\n",
    "            model_7feat[\"metrics\"][\"f1_macro\"],\n",
    "        ],\n",
    "        \"Brier Score\": [\n",
    "            model_8feat[\"metrics\"][\"brier\"],\n",
    "            model_7feat[\"metrics\"][\"brier\"],\n",
    "        ],\n",
    "        \"Optimal Threshold\": [\n",
    "            model_8feat[\"thresholds\"][\"optimal_threshold\"],\n",
    "            model_7feat[\"thresholds\"][\"optimal_threshold\"],\n",
    "        ],\n",
    "        \"Gray-zone Rate\": [\n",
    "            model_8feat[\"thresholds\"][\"gray_zone_rate\"],\n",
    "            model_7feat[\"thresholds\"][\"gray_zone_rate\"],\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(metrics_comp.round(4))\n",
    "\n",
    "# Calculate performance differences\n",
    "pr_auc_drop = model_8feat[\"metrics\"][\"pr_auc\"] - model_7feat[\"metrics\"][\"pr_auc\"]\n",
    "f1_drop = model_8feat[\"metrics\"][\"f1_macro\"] - model_7feat[\"metrics\"][\"f1_macro\"]\n",
    "brier_change = model_7feat[\"metrics\"][\"brier\"] - model_8feat[\"metrics\"][\"brier\"]\n",
    "\n",
    "print(f\"\\nPerformance Impact of Removing IsHTTPS:\")\n",
    "print(\n",
    "    f\"  PR-AUC drop: {pr_auc_drop:.4f} ({pr_auc_drop / model_8feat['metrics']['pr_auc'] * 100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  F1-macro drop: {f1_drop:.4f} ({f1_drop / model_8feat['metrics']['f1_macro'] * 100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Brier score increase: {brier_change:.4f} ({brier_change / model_8feat['metrics']['brier'] * 100:.2f}%)\"\n",
    ")\n",
    "\n",
    "\n",
    "# Helper function to extract feature importances from CalibratedClassifierCV\n",
    "def get_feature_importances_from_calibrated(calibrated_model):\n",
    "    \"\"\"\n",
    "    Extract feature importances from CalibratedClassifierCV object.\n",
    "\n",
    "    CalibratedClassifierCV wraps the base estimator, so we need to access\n",
    "    the base estimator through calibrated_classifiers_[0].estimator\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Access the first calibrated classifier's base estimator\n",
    "        base_estimator = calibrated_model.calibrated_classifiers_[0].estimator\n",
    "\n",
    "        # Get feature importances from the base estimator\n",
    "        if hasattr(base_estimator, \"feature_importances_\"):\n",
    "            return base_estimator.feature_importances_\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                f\"Base estimator {type(base_estimator)} doesn't have feature_importances_\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting feature importances: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Feature importance comparison\n",
    "print(f\"\\nFeature Importance Analysis:\")\n",
    "\n",
    "# Extract feature importances for 8-feature model\n",
    "feat_imp_8_values = get_feature_importances_from_calibrated(model_8feat[\"model\"])\n",
    "if feat_imp_8_values is not None:\n",
    "    feat_imp_8 = pd.DataFrame(\n",
    "        {\"feature\": model_8feat[\"features\"], \"importance\": feat_imp_8_values}\n",
    "    ).sort_values(\"importance\", ascending=False)\n",
    "else:\n",
    "    print(\"Could not extract feature importances for 8-feature model\")\n",
    "    feat_imp_8 = None\n",
    "\n",
    "# Extract feature importances for 7-feature model\n",
    "feat_imp_7_values = get_feature_importances_from_calibrated(model_7feat[\"model\"])\n",
    "if feat_imp_7_values is not None:\n",
    "    feat_imp_7 = pd.DataFrame(\n",
    "        {\"feature\": model_7feat[\"features\"], \"importance\": feat_imp_7_values}\n",
    "    ).sort_values(\"importance\", ascending=False)\n",
    "else:\n",
    "    print(\"Could not extract feature importances for 7-feature model\")\n",
    "    feat_imp_7 = None\n",
    "\n",
    "# Display feature importances if successfully extracted\n",
    "if feat_imp_8 is not None:\n",
    "    print(\"\\n8-Feature Model - Feature Importance:\")\n",
    "    for _, row in feat_imp_8.head(8).iterrows():\n",
    "        print(f\"  {row['feature']:30s}: {row['importance']:.4f}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nIsHTTPS importance rank in 8-feature model: {feat_imp_8[feat_imp_8['feature'] == 'IsHTTPS'].index[0] + 1}/8\"\n",
    "    )\n",
    "    https_importance = feat_imp_8[feat_imp_8[\"feature\"] == \"IsHTTPS\"][\n",
    "        \"importance\"\n",
    "    ].iloc[0]\n",
    "    print(f\"IsHTTPS importance value: {https_importance:.4f}\")\n",
    "\n",
    "if feat_imp_7 is not None:\n",
    "    print(\"\\n7-Feature Model - Feature Importance:\")\n",
    "    for _, row in feat_imp_7.head(7).iterrows():\n",
    "        print(f\"  {row['feature']:30s}: {row['importance']:.4f}\")\n",
    "\n",
    "# Final conclusion\n",
    "print(f\"\\nConclusion:\")\n",
    "if pr_auc_drop < 0.01 and f1_drop < 0.01:\n",
    "    print(\"‚úì IsHTTPS has minimal impact on model performance\")\n",
    "    print(\"‚úì 7-feature model is suitable for production (URL-only features)\")\n",
    "else:\n",
    "    print(\"‚ö† IsHTTPS contributes significantly to model performance\")\n",
    "    print(\"‚ö† Consider keeping IsHTTPS feature if protocol info is available\")\n",
    "\n",
    "print(\n",
    "    f\"\\nRecommendation: {'Use 7-feature model' if pr_auc_drop < 0.01 else 'Use 8-feature model'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3703916e",
   "metadata": {},
   "source": [
    "### **SECTION 7: Model Artifact Persistence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966393dc",
   "metadata": {},
   "source": [
    "#### **SECTION 7.1: Model Artifact Persistence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8aa34418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAVING BOTH MODELS FOR PRODUCTION\n",
      "============================================================\n",
      "\n",
      " Saved 8-feature model:\n",
      "  Model: models\\dev\\model_8feat.pkl\n",
      "  MD5: 051bdaa874f12d709045c1e25f65316f\n",
      "  Metadata: models\\dev\\model_8feat_meta.json\n",
      "\n",
      "‚úì Saved 7-feature model:\n",
      "  Model: models\\dev\\model_7feat.pkl\n",
      "  MD5: a336fd721c1c97ca1f541e48749ee8e2\n",
      "  Metadata: models\\dev\\model_7feat_meta.json\n",
      "\n",
      " Saved thresholds:\n",
      "  8-feature: configs\\dev\\thresholds_8feat.json\n",
      "  7-feature: configs\\dev\\thresholds_7feat.json\n",
      "\n",
      "============================================================\n",
      "ARTIFACTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Research Model (8-feature with IsHTTPS):\n",
      "  - PR-AUC: 0.9992\n",
      "  - Purpose: Maximum performance on this dataset\n",
      "  - Path: models\\dev\\model_8feat.pkl\n",
      "\n",
      "Production Model (7-feature without IsHTTPS):\n",
      "  - PR-AUC: 0.9988  \n",
      "  - Purpose: Robust to 2025 HTTPS phishing landscape\n",
      "  - Path: models\\dev\\model_7feat.pkl\n",
      "  - RECOMMENDED FOR DEPLOYMENT\n",
      "\n",
      "Both models ready for service integration.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SAVING BOTH MODELS FOR PRODUCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "MODEL_DIR = Path(\"models/dev\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save 8-feature model (research baseline)\n",
    "MODEL_8_PATH = MODEL_DIR / \"model_8feat.pkl\"\n",
    "META_8_PATH = MODEL_DIR / \"model_8feat_meta.json\"\n",
    "\n",
    "joblib.dump(best_model, MODEL_8_PATH)\n",
    "\n",
    "meta_8feat = {\n",
    "    \"feature_order\": OPTIMAL_FEATURES,\n",
    "    \"class_mapping\": {\"phish\": 0, \"legit\": 1},\n",
    "    \"phish_proba_col_index\": 0,\n",
    "    \"model_type\": type(best_model).__name__,\n",
    "    \"calibration\": \"isotonic_cv5\",\n",
    "    \"training_date\": pd.Timestamp.now().isoformat(),\n",
    "    \"seed\": SEED,\n",
    "    \"metrics\": model_8feat[\"metrics\"],\n",
    "    \"thresholds\": model_8feat[\"thresholds\"],\n",
    "    \"notes\": f\"8-feature model with IsHTTPS - research baseline, {model_8feat['metrics']['pr_auc']:.2%} PR-AUC\",\n",
    "}\n",
    "\n",
    "META_8_PATH.write_text(json.dumps(meta_8feat, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "model_8_md5 = hashlib.md5(MODEL_8_PATH.read_bytes()).hexdigest()\n",
    "\n",
    "print(f\"\\n Saved 8-feature model:\")\n",
    "print(f\"  Model: {MODEL_8_PATH}\")\n",
    "print(f\"  MD5: {model_8_md5}\")\n",
    "print(f\"  Metadata: {META_8_PATH}\")\n",
    "\n",
    "# Save 7-feature model (production candidate)\n",
    "MODEL_7_PATH = MODEL_DIR / \"model_7feat.pkl\"\n",
    "META_7_PATH = MODEL_DIR / \"model_7feat_meta.json\"\n",
    "\n",
    "joblib.dump(calib_7feat, MODEL_7_PATH)\n",
    "\n",
    "meta_7feat = {\n",
    "    \"feature_order\": FEATURES_7,\n",
    "    \"class_mapping\": {\"phish\": 0, \"legit\": 1},\n",
    "    \"phish_proba_col_index\": 0,\n",
    "    \"model_type\": type(calib_7feat).__name__,\n",
    "    \"calibration\": \"isotonic_cv5\",\n",
    "    \"training_date\": pd.Timestamp.now().isoformat(),\n",
    "    \"seed\": SEED,\n",
    "    \"metrics\": model_7feat[\"metrics\"],\n",
    "    \"thresholds\": model_7feat[\"thresholds\"],\n",
    "    \"notes\": f\"7-feature model without IsHTTPS - production candidate, {model_7feat['metrics']['pr_auc']:.2%} PR-AUC, robust to HTTPS phishing\",\n",
    "}\n",
    "\n",
    "META_7_PATH.write_text(json.dumps(meta_7feat, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "model_7_md5 = hashlib.md5(MODEL_7_PATH.read_bytes()).hexdigest()\n",
    "\n",
    "print(f\"\\n‚úì Saved 7-feature model:\")\n",
    "print(f\"  Model: {MODEL_7_PATH}\")\n",
    "print(f\"  MD5: {model_7_md5}\")\n",
    "print(f\"  Metadata: {META_7_PATH}\")\n",
    "\n",
    "# Save thresholds for both\n",
    "THRESH_8_PATH = Path(\"configs/dev/thresholds_8feat.json\")\n",
    "THRESH_7_PATH = Path(\"configs/dev/thresholds_7feat.json\")\n",
    "\n",
    "with open(THRESH_8_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"model\": \"xgb_8feat\",\n",
    "            \"features\": OPTIMAL_FEATURES,\n",
    "            \"class_mapping\": {\"phish\": 0, \"legit\": 1},\n",
    "            \"calibration\": {\"method\": \"isotonic\", \"cv\": 5},\n",
    "            \"thresholds\": model_8feat[\"thresholds\"],\n",
    "            \"seed\": SEED,\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "with open(THRESH_7_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"model\": \"xgb_7feat\",\n",
    "            \"features\": FEATURES_7,\n",
    "            \"class_mapping\": {\"phish\": 0, \"legit\": 1},\n",
    "            \"calibration\": {\"method\": \"isotonic\", \"cv\": 5},\n",
    "            \"thresholds\": model_7feat[\"thresholds\"],\n",
    "            \"seed\": SEED,\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(f\"\\n Saved thresholds:\")\n",
    "print(f\"  8-feature: {THRESH_8_PATH}\")\n",
    "print(f\"  7-feature: {THRESH_7_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ARTIFACTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Research Model (8-feature with IsHTTPS):\n",
    "  - PR-AUC: {model_8feat[\"metrics\"][\"pr_auc\"]:.4f}\n",
    "  - Purpose: Maximum performance on this dataset\n",
    "  - Path: {MODEL_8_PATH}\n",
    "\n",
    "Production Model (7-feature without IsHTTPS):\n",
    "  - PR-AUC: {model_7feat[\"metrics\"][\"pr_auc\"]:.4f}  \n",
    "  - Purpose: Robust to 2025 HTTPS phishing landscape\n",
    "  - Path: {MODEL_7_PATH}\n",
    "  - RECOMMENDED FOR DEPLOYMENT\n",
    "\n",
    "Both models ready for service integration.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c8af7",
   "metadata": {},
   "source": [
    "#### **SECTION 7.2 MLflow Experiment Logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2266255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MLFLOW EXPERIMENT LOGGING\n",
      "============================================================\n",
      "\n",
      " Logged run: xgb_optimal8_calibrated\n",
      " Saved thresholds: configs\\dev\\thresholds.json\n",
      " MLflow UI: http://localhost:5000\n",
      "üèÉ View run xgb_optimal8_calibrated at: http://localhost:5000/#/experiments/873624361181467123/runs/23b8f04be32940c9a391563680cebede\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/873624361181467123\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MLFLOW EXPERIMENT LOGGING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "mlflow.set_experiment(EXPERIMENT)\n",
    "\n",
    "run_name = f\"{best_name}_optimal8_calibrated\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    # Log parameters (configuration)\n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            \"model_type\": best_name,\n",
    "            \"calibration_method\": \"isotonic\",\n",
    "            \"calibration_cv_folds\": 5,\n",
    "            \"n_features\": len(OPTIMAL_FEATURES),\n",
    "            \"seed\": SEED,\n",
    "            \"train_samples\": len(X_train),\n",
    "            \"val_samples\": len(X_val),\n",
    "            \"data_file\": str(DATA_PATH),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log feature list\n",
    "    for i, feature in enumerate(OPTIMAL_FEATURES, 1):\n",
    "        mlflow.log_param(f\"feature_{i}\", feature)\n",
    "\n",
    "    # Log metrics (performance)\n",
    "    # ...existing code...\n",
    "    # Log metrics (performance)\n",
    "    mlflow.log_metrics(\n",
    "        {\n",
    "            \"val_pr_auc_phish\": best_metrics[\"pr_auc_phish\"],\n",
    "            \"val_f1_macro_at_0.5\": best_metrics[\"f1_macro@0.5_on_p_mal\"],\n",
    "            \"val_brier_phish\": best_metrics[\"brier_phish\"],\n",
    "            \"t_star\": threshold_config[\"optimal_threshold\"],\n",
    "            \"threshold_low\": threshold_config[\"gray_zone_low\"],\n",
    "            \"threshold_high\": threshold_config[\"gray_zone_high\"],\n",
    "            \"gray_zone_rate\": threshold_config[\"gray_zone_rate\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save thresholds JSON artifact\n",
    "    threshold_artifact = {\n",
    "        \"model\": best_name,\n",
    "        \"class_mapping\": {\"phish\": 0, \"legit\": 1},\n",
    "        \"calibration\": {\"method\": \"isotonic\", \"cv\": 5},\n",
    "        \"thresholds\": threshold_config,\n",
    "        \"data\": {\"file\": str(DATA_PATH)},\n",
    "        \"seed\": SEED,\n",
    "    }\n",
    "\n",
    "    with open(THRESH_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(threshold_artifact, f, indent=2)\n",
    "\n",
    "    mlflow.log_artifact(str(THRESH_PATH))\n",
    "\n",
    "    print(f\"\\n Logged run: {run_name}\")\n",
    "    print(f\" Saved thresholds: {THRESH_PATH}\")\n",
    "    print(f\" MLflow UI: {MLFLOW_URI}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
