{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8861dc",
   "metadata": {},
   "source": [
    "### **Understanding the Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b62e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a few URLs to manually verify what's extractable\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE URL INSPECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get 3 phishing and 3 legitimate URLs\n",
    "phish_samples = df[df[\"label\"] == 0].head(3)\n",
    "legit_samples = df[df[\"label\"] == 1].head(3)\n",
    "\n",
    "\n",
    "def inspect_url(row):\n",
    "    \"\"\"Show what we can extract from URL alone\"\"\"\n",
    "    url = row[\"URL\"]\n",
    "    print(f\"\\nURL: {url}\")\n",
    "    print(f\"Label: {'PHISH' if row['label'] == 0 else 'LEGIT'}\")\n",
    "    print(f\"-\" * 60)\n",
    "\n",
    "    # URL-only features\n",
    "    print(f\"  URLLength (dataset): {row['URLLength']}\")\n",
    "    print(f\"  url_len (computed):  {len(url)}\")\n",
    "    print(f\"  Match: {row['URLLength'] == len(url)}\")\n",
    "\n",
    "    print(f\"\\n  DomainLength (dataset): {row['DomainLength']}\")\n",
    "    print(f\"  Domain (dataset): {row['Domain']}\")\n",
    "\n",
    "    print(f\"\\n  NoOfSubDomain (dataset): {row['NoOfSubDomain']}\")\n",
    "    print(f\"  TLD: {row['TLD']}\")\n",
    "    print(f\"  TLDLength: {row['TLDLength']}\")\n",
    "\n",
    "    print(f\"\\n  IsHTTPS: {row['IsHTTPS']}\")\n",
    "    print(f\"  IsDomainIP: {row['IsDomainIP']}\")\n",
    "\n",
    "    print(f\"\\n  Character features:\")\n",
    "    print(f\"    NoOfLettersInURL: {row['NoOfLettersInURL']}\")\n",
    "    print(f\"    NoOfDegitsInURL: {row['NoOfDegitsInURL']}\")\n",
    "    print(f\"    DegitRatioInURL: {row['DegitRatioInURL']:.3f}\")\n",
    "    print(f\"    NoOfOtherSpecialCharsInURL: {row['NoOfOtherSpecialCharsInURL']}\")\n",
    "\n",
    "    # Page-content features (should vary widely, show they need fetch)\n",
    "    print(f\"\\n  Page-content features (require fetch):\")\n",
    "    print(f\"    LineOfCode: {row['LineOfCode']}\")\n",
    "    print(f\"    HasTitle: {row['HasTitle']}\")\n",
    "    print(f\"    NoOfImage: {row['NoOfImage']}\")\n",
    "\n",
    "\n",
    "print(\"\\nüé£ PHISHING SAMPLES:\")\n",
    "for idx, row in phish_samples.iterrows():\n",
    "    inspect_url(row)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\n LEGITIMATE SAMPLES:\")\n",
    "for idx, row in legit_samples.iterrows():\n",
    "    inspect_url(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb6247c",
   "metadata": {},
   "source": [
    "- URLLENGTH doesnt have consistent characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fbe40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"URL LENGTH DISCREPANCY INVESTIGATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check first 20 rows systematically\n",
    "mismatches = []\n",
    "\n",
    "for idx in range(20):\n",
    "    row = df.iloc[idx]\n",
    "    url = row[\"URL\"]\n",
    "    dataset_len = row[\"URLLength\"]\n",
    "    computed_len = len(url)\n",
    "    diff = computed_len - dataset_len\n",
    "\n",
    "    if diff != 0:\n",
    "        mismatches.append(\n",
    "            {\n",
    "                \"idx\": idx,\n",
    "                \"url\": url,\n",
    "                \"dataset_len\": dataset_len,\n",
    "                \"computed_len\": computed_len,\n",
    "                \"diff\": diff,\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(f\"\\nFound {len(mismatches)} mismatches in first 20 rows\\n\")\n",
    "\n",
    "for m in mismatches[:5]:  # Show first 5\n",
    "    print(f\"[{m['idx']}] Diff: {m['diff']}\")\n",
    "    print(f\"  URL: {m['url']}\")\n",
    "    print(f\"  Dataset: {m['dataset_len']} | Computed: {m['computed_len']}\")\n",
    "    print()\n",
    "\n",
    "# Full dataset check\n",
    "df[\"computed_url_len\"] = df[\"URL\"].apply(len)\n",
    "df[\"length_diff\"] = df[\"computed_url_len\"] - df[\"URLLength\"]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FULL DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDifference distribution:\")\n",
    "print(df[\"length_diff\"].value_counts().sort_index().head(10))\n",
    "\n",
    "print(f\"\\nPercentage exact match: {(df['length_diff'] == 0).mean() * 100:.2f}%\")\n",
    "print(f\"Percentage off by +1: {(df['length_diff'] == 1).mean() * 100:.2f}%\")\n",
    "print(f\"Percentage off by -1: {(df['length_diff'] == -1).mean() * 100:.2f}%\")\n",
    "print(f\"Percentage off by 2+: {(df['length_diff'].abs() >= 2).mean() * 100:.2f}%\")\n",
    "\n",
    "# Check if it's consistent per protocol\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BY PROTOCOL\")\n",
    "print(\"=\" * 60)\n",
    "df[\"protocol\"] = df[\"URL\"].str.extract(r\"^(https?://)\")\n",
    "print(df.groupby(\"protocol\")[\"length_diff\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f56d3",
   "metadata": {},
   "source": [
    "- Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYZING THE 20% THAT MATCH EXACTLY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get samples of exact matches\n",
    "exact_matches = df[df[\"length_diff\"] == 0].head(10)\n",
    "off_by_one = df[df[\"length_diff\"] == 1].head(10)\n",
    "\n",
    "print(\"\\n‚úÖ EXACT MATCHES (Dataset URLLength = Computed):\")\n",
    "for idx, row in exact_matches.iterrows():\n",
    "    url = row[\"URL\"]\n",
    "    print(f\"  {url} (len={len(url)})\")\n",
    "\n",
    "print(\"\\n‚ùå OFF BY ONE (Dataset URLLength = Computed - 1):\")\n",
    "for idx, row in off_by_one.iterrows():\n",
    "    url = row[\"URL\"]\n",
    "    print(f\"  {url} (len={len(url)})\")\n",
    "\n",
    "# Look for patterns\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PATTERN ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if trailing slashes matter\n",
    "df[\"has_trailing_slash\"] = df[\"URL\"].str.endswith(\"/\")\n",
    "print(\"\\nTrailing slash distribution:\")\n",
    "print(df.groupby(\"has_trailing_slash\")[\"length_diff\"].value_counts())\n",
    "\n",
    "# Check if query strings matter\n",
    "df[\"has_query\"] = df[\"URL\"].str.contains(r\"\\?\")\n",
    "print(\"\\nQuery string distribution:\")\n",
    "print(df.groupby(\"has_query\")[\"length_diff\"].value_counts())\n",
    "\n",
    "# Check URL length ranges\n",
    "print(\"\\nLength difference by URL length bucket:\")\n",
    "df[\"url_len_bucket\"] = pd.cut(\n",
    "    df[\"computed_url_len\"], bins=[0, 30, 50, 100, 200, 500, 10000]\n",
    ")\n",
    "print(df.groupby(\"url_len_bucket\")[\"length_diff\"].value_counts().head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFY OTHER ENGINEERED FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if url_digit_ratio matches DegitRatioInURL\n",
    "# First, compute it fresh\n",
    "df[\"computed_digit_ratio\"] = df[\"URL\"].apply(\n",
    "    lambda url: sum(c.isdigit() for c in url) / len(url) if url else 0\n",
    ")\n",
    "\n",
    "# Compare\n",
    "df[\"digit_ratio_diff\"] = (df[\"computed_digit_ratio\"] - df[\"DegitRatioInURL\"]).abs()\n",
    "\n",
    "print(\"\\nDigit Ratio Comparison:\")\n",
    "print(\n",
    "    f\"  Exact matches: {(df['digit_ratio_diff'] < 0.001).sum():,} ({(df['digit_ratio_diff'] < 0.001).mean() * 100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Close (< 0.01): {(df['digit_ratio_diff'] < 0.01).sum():,} ({(df['digit_ratio_diff'] < 0.01).mean() * 100:.2f}%)\"\n",
    ")\n",
    "print(f\"  Max difference: {df['digit_ratio_diff'].max():.6f}\")\n",
    "\n",
    "# Sample of matches\n",
    "print(\"\\nSample digit ratio comparison:\")\n",
    "for idx in range(5):\n",
    "    row = df.iloc[idx]\n",
    "    print(f\"  URL: {row['URL'][:50]}...\")\n",
    "    print(\n",
    "        f\"    Dataset: {row['DegitRatioInURL']:.4f} | Computed: {row['computed_digit_ratio']:.4f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Check subdomain count\n",
    "# Extract from URL\n",
    "def compute_subdomains(url):\n",
    "    if not isinstance(url, str) or not url:\n",
    "        return 0\n",
    "    host = url.split(\"://\", 1)[-1].split(\"/\", 1)[0]\n",
    "    return max(0, host.count(\".\") - 1)\n",
    "\n",
    "\n",
    "df[\"computed_subdomains\"] = df[\"URL\"].apply(compute_subdomains)\n",
    "df[\"subdomain_diff\"] = (df[\"computed_subdomains\"] - df[\"NoOfSubDomain\"]).abs()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Subdomain Count Comparison:\")\n",
    "print(\n",
    "    f\"  Exact matches: {(df['subdomain_diff'] == 0).sum():,} ({(df['subdomain_diff'] == 0).mean() * 100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Off by 1: {(df['subdomain_diff'] == 1).sum():,} ({(df['subdomain_diff'] == 1).mean() * 100:.2f}%)\"\n",
    ")\n",
    "print(f\"  Max difference: {df['subdomain_diff'].max()}\")\n",
    "\n",
    "# Sample\n",
    "print(\"\\nSample subdomain comparison:\")\n",
    "for idx in range(5):\n",
    "    row = df.iloc[idx]\n",
    "    print(f\"  URL: {row['URL'][:50]}...\")\n",
    "    print(\n",
    "        f\"    Dataset: {row['NoOfSubDomain']} | Computed: {row['computed_subdomains']}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c6f3a",
   "metadata": {},
   "source": [
    "### **Remove Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126271af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REMOVING DUPLICATE URLs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nBefore deduplication:\")\n",
    "print(f\"  Total rows: {len(df):,}\")\n",
    "print(f\"  Unique URLs: {df['URL'].nunique():,}\")\n",
    "\n",
    "# Keep first occurrence of each URL\n",
    "df_deduped = df.drop_duplicates(subset=[\"URL\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nAfter deduplication:\")\n",
    "print(f\"  Total rows: {len(df_deduped):,}\")\n",
    "print(f\"  Unique URLs: {df_deduped['URL'].nunique():,}\")\n",
    "print(f\"  Removed: {len(df) - len(df_deduped):,} duplicate rows\")\n",
    "\n",
    "# Verify label distribution didn't change significantly\n",
    "print(f\"\\nLabel distribution before:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "print(f\"\\nLabel distribution after:\")\n",
    "print(df_deduped[\"label\"].value_counts())\n",
    "\n",
    "# Update working dataframe\n",
    "df = df_deduped.copy()\n",
    "\n",
    "print(\"\\n‚úì Working with deduplicated dataset from now on\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf2c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
